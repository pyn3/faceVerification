{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 2070\n",
      "GPU Properties:\n",
      " _CudaDeviceProperties(name='NVIDIA GeForce RTX 2070', major=7, minor=5, total_memory=8191MB, multi_processor_count=36)\n",
      "Sat Feb 18 17:31:43 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.98       Driver Version: 526.98       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   43C    P2    55W / 175W |    316MiB /  8192MiB |     36%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1688      C   ...ython\\Python39\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      6672    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      8584    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
      "|    0   N/A  N/A      8664    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A      9944    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A     10068    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10408    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     12016    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     12632    C+G   ...e\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13236    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     13248    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print('Python version:', sys.version)\n",
    "print('CUDA Available:', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU Name:', torch.cuda.get_device_name())\n",
    "    print('GPU Properties:\\n', torch.cuda.get_device_properties('cuda'))\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95, 0)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Cuda is not available, please use cpu instead\")\n",
    "    device = \"cpu\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 41\n",
    "# Define custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, df = None, num_sample = None, transform = None, num_img_pool = 10):\n",
    "        # set random seed for FaceDataset\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        # create constructors\n",
    "        self.unique_img_name = None\n",
    "        self.data = dict()\n",
    "        self.images = list()\n",
    "        # label to indices\n",
    "        self.label_to_indices = dict()\n",
    "        self.labels = list()\n",
    "        # read csv file\n",
    "        self.df = df\n",
    "        # set the transformation\n",
    "        self.transform = transform\n",
    "        # drop last n row from dataframe\n",
    "        self.df = self.df.head(num_sample)\n",
    "        #get the length of entire dataset\n",
    "        self.len_ = len(self.df)\n",
    "        # load imgs\n",
    "        self.load_imgs(self.df, num_imgs = num_img_pool, max = num_sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "    # get each pair of images -> 1: same identity, 0: different identity\n",
    "    # if index is even -> same pair\n",
    "    # if index is odd -> random identity\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img = self.images[idx]\n",
    "        anchor_label = self.labels[idx]\n",
    "\n",
    "        pos_idx = np.random.choice(np.arange(len(self.images))[self.labels == anchor_label])\n",
    "        neg_idx = np.random.choice(np.arange(len(self.images))[self.labels != anchor_label])\n",
    "\n",
    "        pos_img = self.images[pos_idx]\n",
    "        neg_img = self.images[neg_idx]\n",
    "\n",
    "        pos_label = self.labels[pos_idx]\n",
    "        neg_label = self.labels[neg_idx]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            anchor_img = img_to_tensor(anchor_img)\n",
    "            pos_img = img_to_tensor(pos_img)\n",
    "            neg_img = img_to_tensor(neg_img)\n",
    "        else:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            pos_img = self.transform(pos_img)\n",
    "            neg_img = self.transform(neg_img)\n",
    "\n",
    "        return anchor_img, pos_img, neg_img\n",
    "\n",
    "    # load imgs from pandas to memory and define the maximum number of images\n",
    "    def load_imgs(self, df, num_imgs, max):\n",
    "        # iterate thought each row\n",
    "        for i, row in tqdm(df.iterrows(), total = max):\n",
    "            # get identity of each row\n",
    "            row_identity = row['identity']\n",
    "            # append each identity to numberical value\n",
    "            self.label_to_indices[int(row_identity)] = i\n",
    "            count_img = 0\n",
    "            # loop imgs in each identity\n",
    "            for img_name in row['path']:\n",
    "                if count_img > num_imgs:\n",
    "                    break\n",
    "                # concatenate the directoru and image name\n",
    "                # path_to_image = self.dir+img_name\n",
    "                path_to_image = img_name\n",
    "                # open image and convert to RGB\n",
    "                img = Image.open(path_to_image).convert('RGB')\n",
    "\n",
    "                self.images.append(img)\n",
    "                self.labels.append(i)\n",
    "                count_img += 1\n",
    "            # print('Added img '+ str(row_identity))\n",
    "        self.labels = np.array(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[digiFace1M\\subjects_0-1999_72_imgs\\0\\20.png, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[digiFace1M\\subjects_0-1999_72_imgs\\1\\66.png, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[digiFace1M\\subjects_0-1999_72_imgs\\2\\29.png, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[digiFace1M\\subjects_0-1999_72_imgs\\3\\42.png, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[digiFace1M\\subjects_0-1999_72_imgs\\4\\33.png, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72661</th>\n",
       "      <td>199994</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72662</th>\n",
       "      <td>199995</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72663</th>\n",
       "      <td>199996</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72664</th>\n",
       "      <td>199997</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1999...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72665</th>\n",
       "      <td>199998</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1999...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72666 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       identity                                               path\n",
       "0             0  [digiFace1M\\subjects_0-1999_72_imgs\\0\\20.png, ...\n",
       "1             1  [digiFace1M\\subjects_0-1999_72_imgs\\1\\66.png, ...\n",
       "2             2  [digiFace1M\\subjects_0-1999_72_imgs\\2\\29.png, ...\n",
       "3             3  [digiFace1M\\subjects_0-1999_72_imgs\\3\\42.png, ...\n",
       "4             4  [digiFace1M\\subjects_0-1999_72_imgs\\4\\33.png, ...\n",
       "...         ...                                                ...\n",
       "72661    199994  [digiFace1M\\subjects_166666-199998_5_imgs\\1999...\n",
       "72662    199995  [digiFace1M\\subjects_166666-199998_5_imgs\\1999...\n",
       "72663    199996  [digiFace1M\\subjects_166666-199998_5_imgs\\1999...\n",
       "72664    199997  [digiFace1M\\subjects_166666-199998_5_imgs\\1999...\n",
       "72665    199998  [digiFace1M\\subjects_166666-199998_5_imgs\\1999...\n",
       "\n",
       "[72666 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_df = pd.read_csv('./digiface_csv_files/digi_all.csv')\n",
    "ds_df = ds_df.groupby('identity')['path'].apply(list).reset_index()\n",
    "ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Size: 50866\n",
      "Val Size: 13080\n",
      "Test Size: 8720\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>identity</th>\n",
       "      <th>path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26344</th>\n",
       "      <td>120344</td>\n",
       "      <td>[digiFace1M\\subjects_100000-133332_5_imgs\\1203...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61833</th>\n",
       "      <td>189166</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1891...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46921</th>\n",
       "      <td>174254</td>\n",
       "      <td>[digiFace1M\\subjects_166666-199998_5_imgs\\1742...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4873</th>\n",
       "      <td>8873</td>\n",
       "      <td>[digiFace1M\\subjects_8000-9999_72_imgs\\8873\\54...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19875</th>\n",
       "      <td>113875</td>\n",
       "      <td>[digiFace1M\\subjects_100000-133332_5_imgs\\1138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       identity                                               path\n",
       "26344    120344  [digiFace1M\\subjects_100000-133332_5_imgs\\1203...\n",
       "61833    189166  [digiFace1M\\subjects_166666-199998_5_imgs\\1891...\n",
       "46921    174254  [digiFace1M\\subjects_166666-199998_5_imgs\\1742...\n",
       "4873       8873  [digiFace1M\\subjects_8000-9999_72_imgs\\8873\\54...\n",
       "19875    113875  [digiFace1M\\subjects_100000-133332_5_imgs\\1138..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting each dataset\n",
    "train_df, eval_df = train_test_split(ds_df, test_size= 0.3, shuffle = True, random_state = seed)\n",
    "val_df, test_df = train_test_split(eval_df, test_size = 0.4, shuffle = True, random_state = seed)\n",
    "\n",
    "# print to check size of each dataset\n",
    "print(f'Train Size: {len(train_df)}')\n",
    "print(f'Val Size: {len(val_df)}')\n",
    "print(f'Test Size: {len(test_df)}')\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size\n",
    "img_size = 112\n",
    "\n",
    "# # define transformation for test set\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.CenterCrop(img_size),\n",
    "#     transforms.RandomHorizontalFlip(p=0.6),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# # define transformation for validation set\n",
    "# val_transform = transforms.Compose([\n",
    "#     transforms.Resize((256, 256)),\n",
    "#     transforms.CenterCrop(img_size),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                          std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "\n",
    "# # define batch size\n",
    "# train_batch_size = 64\n",
    "# val_batch_size = 64\n",
    "# print('------------Started Loading Train Set------------')\n",
    "# # create dataloader for train set\n",
    "# train_triplet_dataset = FaceDataset(df = train_df, num_sample = 6000, transform = train_transform)\n",
    "# train_triplet_dataloader = DataLoader(train_triplet_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "# print('Total Train Set: ', train_triplet_dataset.__len__())\n",
    "# print('-----------Finished Loading Train Set------------')\n",
    "\n",
    "# print('\\n')\n",
    "\n",
    "# print('------------Started Loading Validation Set------------')\n",
    "# # create dataloader for validation set\n",
    "# val_triplet_dataset = FaceDataset(df = val_df,num_sample = 3000, transform = val_transform)\n",
    "# val_triplet_dataloader = DataLoader(val_triplet_dataset, batch_size=val_batch_size, shuffle=True, pin_memory=True)\n",
    "# print('Total Train Set: ', val_triplet_dataset.__len__())\n",
    "# print('-----------Finished Loading Validation Set------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, ReLU6, Sigmoid, Dropout2d\\\n",
    "    ,Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class h_sigmoid(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
    "        self.fc = Sequential(\n",
    "            Linear(channel, channel // reduction),\n",
    "            ReLU(inplace=True),\n",
    "            Linear(channel // reduction, channel),\n",
    "            h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class PermutationBlock(Module):\n",
    "    def __init__(self, groups):\n",
    "        super(PermutationBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        G = self.groups\n",
    "        output = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n",
    "        return output\n",
    "\n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=nr_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1\n",
    "\n",
    "class DropBlock2D(nn.Module):\n",
    "    r\"\"\"Randomly zeroes 2D spatial blocks of the input tensor.\n",
    "    As described in the paper\n",
    "    `DropBlock: A regularization method for convolutional networks`_ ,\n",
    "    dropping whole blocks of feature map allows to remove semantic\n",
    "    information as compared to regular dropout.\n",
    "    Args:\n",
    "        drop_prob (float): probability of an element to be dropped.\n",
    "        block_size (int): size of the block to drop\n",
    "    Shape:\n",
    "        - Input: `(N, C, H, W)`\n",
    "        - Output: `(N, C, H, W)`\n",
    "    .. _DropBlock: A regularization method for convolutional networks:\n",
    "       https://arxiv.org/abs/1810.12890\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size=512):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "        self.dropblock = DropBlock2D(block_size=3, drop_prob=0.3)\n",
    "        self.dropout = Dropout2d(0.1)\n",
    "\n",
    "    def forward_once(self,x):\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.conv2_dw(out)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "\n",
    "        out = self.conv_34(out)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "\n",
    "        # out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "\n",
    "        out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "\n",
    "        out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        return l2_norm(out)\n",
    "\n",
    "    def forward(self, anchor_img, positive_img, negative_img):\n",
    "        anchor = self.forward_once(anchor_img)\n",
    "        positive = self.forward_once(positive_img)\n",
    "        negative = self.forward_once(negative_img)\n",
    "        return anchor, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "MobileFaceNet                                 [32, 512]                 --\n",
      "├─Conv_block: 1-1                             [32, 64, 56, 56]          --\n",
      "│    └─Conv2d: 2-1                            [32, 64, 56, 56]          1,728\n",
      "│    └─BatchNorm2d: 2-2                       [32, 64, 56, 56]          128\n",
      "│    └─PReLU: 2-3                             [32, 64, 56, 56]          64\n",
      "├─Conv_block: 1-2                             [32, 64, 56, 56]          --\n",
      "│    └─Conv2d: 2-4                            [32, 64, 56, 56]          576\n",
      "│    └─BatchNorm2d: 2-5                       [32, 64, 56, 56]          128\n",
      "│    └─PReLU: 2-6                             [32, 64, 56, 56]          64\n",
      "├─Depth_Wise: 1-3                             [32, 64, 28, 28]          --\n",
      "│    └─Conv_block: 2-7                        [32, 128, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-1                       [32, 128, 56, 56]         8,192\n",
      "│    │    └─BatchNorm2d: 3-2                  [32, 128, 56, 56]         256\n",
      "│    │    └─PReLU: 3-3                        [32, 128, 56, 56]         128\n",
      "│    └─Conv_block: 2-8                        [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-4                       [32, 128, 28, 28]         1,152\n",
      "│    │    └─BatchNorm2d: 3-5                  [32, 128, 28, 28]         256\n",
      "│    │    └─PReLU: 3-6                        [32, 128, 28, 28]         128\n",
      "│    └─Linear_block: 2-9                      [32, 64, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-7                       [32, 64, 28, 28]          8,192\n",
      "│    │    └─BatchNorm2d: 3-8                  [32, 64, 28, 28]          128\n",
      "├─Residual: 1-4                               [32, 64, 28, 28]          --\n",
      "│    └─Sequential: 2-10                       [32, 64, 28, 28]          --\n",
      "│    │    └─Depth_Wise: 3-9                   [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-10                  [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-11                  [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-12                  [32, 64, 28, 28]          18,432\n",
      "├─Depth_Wise: 1-5                             [32, 128, 14, 14]         --\n",
      "│    └─Conv_block: 2-11                       [32, 256, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-13                      [32, 256, 28, 28]         16,384\n",
      "│    │    └─BatchNorm2d: 3-14                 [32, 256, 28, 28]         512\n",
      "│    │    └─PReLU: 3-15                       [32, 256, 28, 28]         256\n",
      "│    └─Conv_block: 2-12                       [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-16                      [32, 256, 14, 14]         2,304\n",
      "│    │    └─BatchNorm2d: 3-17                 [32, 256, 14, 14]         512\n",
      "│    │    └─PReLU: 3-18                       [32, 256, 14, 14]         256\n",
      "│    └─Linear_block: 2-13                     [32, 128, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-19                      [32, 128, 14, 14]         32,768\n",
      "│    │    └─BatchNorm2d: 3-20                 [32, 128, 14, 14]         256\n",
      "├─Residual: 1-6                               [32, 128, 14, 14]         --\n",
      "│    └─Sequential: 2-14                       [32, 128, 14, 14]         --\n",
      "│    │    └─Depth_Wise: 3-21                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-22                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-23                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-24                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-25                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-26                  [32, 128, 14, 14]         69,632\n",
      "├─Depth_Wise: 1-7                             [32, 128, 7, 7]           --\n",
      "│    └─Conv_block: 2-15                       [32, 512, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-27                      [32, 512, 14, 14]         65,536\n",
      "│    │    └─BatchNorm2d: 3-28                 [32, 512, 14, 14]         1,024\n",
      "│    │    └─PReLU: 3-29                       [32, 512, 14, 14]         512\n",
      "│    └─Conv_block: 2-16                       [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-30                      [32, 512, 7, 7]           4,608\n",
      "│    │    └─BatchNorm2d: 3-31                 [32, 512, 7, 7]           1,024\n",
      "│    │    └─PReLU: 3-32                       [32, 512, 7, 7]           512\n",
      "│    └─Linear_block: 2-17                     [32, 128, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-33                      [32, 128, 7, 7]           65,536\n",
      "│    │    └─BatchNorm2d: 3-34                 [32, 128, 7, 7]           256\n",
      "├─Residual: 1-8                               [32, 128, 7, 7]           --\n",
      "│    └─Sequential: 2-18                       [32, 128, 7, 7]           --\n",
      "│    │    └─Depth_Wise: 3-35                  [32, 128, 7, 7]           69,632\n",
      "│    │    └─Depth_Wise: 3-36                  [32, 128, 7, 7]           69,632\n",
      "├─Conv_block: 1-9                             [32, 512, 7, 7]           --\n",
      "│    └─Conv2d: 2-19                           [32, 512, 7, 7]           65,536\n",
      "│    └─BatchNorm2d: 2-20                      [32, 512, 7, 7]           1,024\n",
      "│    └─PReLU: 2-21                            [32, 512, 7, 7]           512\n",
      "├─DropBlock2D: 1-10                           [32, 512, 7, 7]           --\n",
      "├─Linear_block: 1-11                          [32, 512, 1, 1]           --\n",
      "│    └─Conv2d: 2-22                           [32, 512, 1, 1]           25,088\n",
      "│    └─BatchNorm2d: 2-23                      [32, 512, 1, 1]           1,024\n",
      "├─DropBlock2D: 1-12                           [32, 512, 1, 1]           --\n",
      "├─Flatten: 1-13                               [32, 512]                 --\n",
      "├─Linear: 1-14                                [32, 512]                 262,144\n",
      "├─BatchNorm1d: 1-15                           [32, 512]                 1,024\n",
      "├─Conv_block: 1-16                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-24                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-25                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-26                            [32, 64, 56, 56]          (recursive)\n",
      "├─Conv_block: 1-17                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-27                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-28                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-29                            [32, 64, 56, 56]          (recursive)\n",
      "├─Depth_Wise: 1-18                            [32, 64, 28, 28]          (recursive)\n",
      "│    └─Conv_block: 2-30                       [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─Conv2d: 3-37                      [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-38                 [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─PReLU: 3-39                       [32, 128, 56, 56]         (recursive)\n",
      "│    └─Conv_block: 2-31                       [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-40                      [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-41                 [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-42                       [32, 128, 28, 28]         (recursive)\n",
      "│    └─Linear_block: 2-32                     [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Conv2d: 3-43                      [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─BatchNorm2d: 3-44                 [32, 64, 28, 28]          (recursive)\n",
      "├─Residual: 1-19                              [32, 64, 28, 28]          (recursive)\n",
      "│    └─Sequential: 2-33                       [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-45                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-46                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-47                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-48                  [32, 64, 28, 28]          (recursive)\n",
      "├─Depth_Wise: 1-20                            [32, 128, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-34                       [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-49                      [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-50                 [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-51                       [32, 256, 28, 28]         (recursive)\n",
      "│    └─Conv_block: 2-35                       [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-52                      [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-53                 [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-54                       [32, 256, 14, 14]         (recursive)\n",
      "│    └─Linear_block: 2-36                     [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-55                      [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-56                 [32, 128, 14, 14]         (recursive)\n",
      "├─Residual: 1-21                              [32, 128, 14, 14]         (recursive)\n",
      "│    └─Sequential: 2-37                       [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-57                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-58                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-59                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-60                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-61                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-62                  [32, 128, 14, 14]         (recursive)\n",
      "├─Depth_Wise: 1-22                            [32, 128, 7, 7]           (recursive)\n",
      "│    └─Conv_block: 2-38                       [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-63                      [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-64                 [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-65                       [32, 512, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-39                       [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-66                      [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-67                 [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─PReLU: 3-68                       [32, 512, 7, 7]           (recursive)\n",
      "│    └─Linear_block: 2-40                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-69                      [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-70                 [32, 128, 7, 7]           (recursive)\n",
      "├─Residual: 1-23                              [32, 128, 7, 7]           (recursive)\n",
      "│    └─Sequential: 2-41                       [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-71                  [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-72                  [32, 128, 7, 7]           (recursive)\n",
      "├─Conv_block: 1-24                            [32, 512, 7, 7]           (recursive)\n",
      "│    └─Conv2d: 2-42                           [32, 512, 7, 7]           (recursive)\n",
      "│    └─BatchNorm2d: 2-43                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─PReLU: 2-44                            [32, 512, 7, 7]           (recursive)\n",
      "├─DropBlock2D: 1-25                           [32, 512, 7, 7]           --\n",
      "├─Linear_block: 1-26                          [32, 512, 1, 1]           (recursive)\n",
      "│    └─Conv2d: 2-45                           [32, 512, 1, 1]           (recursive)\n",
      "│    └─BatchNorm2d: 2-46                      [32, 512, 1, 1]           (recursive)\n",
      "├─DropBlock2D: 1-27                           [32, 512, 1, 1]           --\n",
      "├─Flatten: 1-28                               [32, 512]                 --\n",
      "├─Linear: 1-29                                [32, 512]                 (recursive)\n",
      "├─BatchNorm1d: 1-30                           [32, 512]                 (recursive)\n",
      "├─Conv_block: 1-31                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-47                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-48                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-49                            [32, 64, 56, 56]          (recursive)\n",
      "├─Conv_block: 1-32                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-50                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-51                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-52                            [32, 64, 56, 56]          (recursive)\n",
      "├─Depth_Wise: 1-33                            [32, 64, 28, 28]          (recursive)\n",
      "│    └─Conv_block: 2-53                       [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─Conv2d: 3-73                      [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-74                 [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─PReLU: 3-75                       [32, 128, 56, 56]         (recursive)\n",
      "│    └─Conv_block: 2-54                       [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-76                      [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-77                 [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-78                       [32, 128, 28, 28]         (recursive)\n",
      "│    └─Linear_block: 2-55                     [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Conv2d: 3-79                      [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─BatchNorm2d: 3-80                 [32, 64, 28, 28]          (recursive)\n",
      "├─Residual: 1-34                              [32, 64, 28, 28]          (recursive)\n",
      "│    └─Sequential: 2-56                       [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-81                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-82                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-83                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-84                  [32, 64, 28, 28]          (recursive)\n",
      "├─Depth_Wise: 1-35                            [32, 128, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-57                       [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-85                      [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-86                 [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-87                       [32, 256, 28, 28]         (recursive)\n",
      "│    └─Conv_block: 2-58                       [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-88                      [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-89                 [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-90                       [32, 256, 14, 14]         (recursive)\n",
      "│    └─Linear_block: 2-59                     [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-91                      [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-92                 [32, 128, 14, 14]         (recursive)\n",
      "├─Residual: 1-36                              [32, 128, 14, 14]         (recursive)\n",
      "│    └─Sequential: 2-60                       [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-93                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-94                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-95                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-96                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-97                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-98                  [32, 128, 14, 14]         (recursive)\n",
      "├─Depth_Wise: 1-37                            [32, 128, 7, 7]           (recursive)\n",
      "│    └─Conv_block: 2-61                       [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-99                      [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-100                [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-101                      [32, 512, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-62                       [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-102                     [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-103                [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─PReLU: 3-104                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─Linear_block: 2-63                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-105                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-106                [32, 128, 7, 7]           (recursive)\n",
      "├─Residual: 1-38                              [32, 128, 7, 7]           (recursive)\n",
      "│    └─Sequential: 2-64                       [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-107                 [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-108                 [32, 128, 7, 7]           (recursive)\n",
      "├─Conv_block: 1-39                            [32, 512, 7, 7]           (recursive)\n",
      "│    └─Conv2d: 2-65                           [32, 512, 7, 7]           (recursive)\n",
      "│    └─BatchNorm2d: 2-66                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─PReLU: 2-67                            [32, 512, 7, 7]           (recursive)\n",
      "├─DropBlock2D: 1-40                           [32, 512, 7, 7]           --\n",
      "├─Linear_block: 1-41                          [32, 512, 1, 1]           (recursive)\n",
      "│    └─Conv2d: 2-68                           [32, 512, 1, 1]           (recursive)\n",
      "│    └─BatchNorm2d: 2-69                      [32, 512, 1, 1]           (recursive)\n",
      "├─DropBlock2D: 1-42                           [32, 512, 1, 1]           --\n",
      "├─Flatten: 1-43                               [32, 512]                 --\n",
      "├─Linear: 1-44                                [32, 512]                 (recursive)\n",
      "├─BatchNorm1d: 1-45                           [32, 512]                 (recursive)\n",
      "===============================================================================================\n",
      "Total params: 1,200,512\n",
      "Trainable params: 1,200,512\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 21.23\n",
      "===============================================================================================\n",
      "Input size (MB): 14.45\n",
      "Forward/backward pass size (MB): 2347.96\n",
      "Params size (MB): 4.80\n",
      "Estimated Total Size (MB): 2367.21\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# triplet_model = TripletNetwork()\n",
    "triplet_model = MobileFaceNet()\n",
    "print(summary(triplet_model, input_size=[(32,3,112,112),(32,3,112,112),(32,3,112,112)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        # calculate Euclidean's distance\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        # distance_positive = F.pairwise_distance(anchor, positive).view((-1, 1))\n",
    "        # distance_negative = F.pairwise_distance(anchor, negative).view((-1, 1))\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return torch.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "\n",
    "    acc = float(tp + tn) / dist.size\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'margin': 1.2,\n",
    "    'lr': 1e-3,\n",
    "    'patience': 5,\n",
    "    'factor': 0.1,\n",
    "    'min_lr': 1e-10,\n",
    "    'threshold': 1e-3\n",
    "}\n",
    "criterion = TripletLoss(margin = model_config['margin'])\n",
    "optimizer = optim.Adam(triplet_model.parameters(), lr=model_config['lr'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=model_config['patience'], factor=model_config['factor'], min_lr=model_config['min_lr'], threshold=model_config['threshold'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def train_model(model, train_loader, train_optimizer, train_criterion):\n",
    "    train_loss = 0\n",
    "    train_accuracy = list()\n",
    "    # set to train mode\n",
    "    model.train()\n",
    "    # enable grad\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # loop though loader\n",
    "        for anchor_img, pos_img, neg_img in train_loader:\n",
    "            # for calculate accuracy\n",
    "            threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "            is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "            # throw img to CUDA\n",
    "            anchor_img, pos_img,neg_img = anchor_img.to(device), pos_img.to(device),neg_img.to(device)\n",
    "            # prevent grad vansishing\n",
    "            train_optimizer.zero_grad()\n",
    "            # training model\n",
    "            anchor_img, pos_img, neg_img = model(anchor_img, pos_img,neg_img)\n",
    "            # calculate loss\n",
    "            loss = train_criterion(anchor_img, pos_img, neg_img)\n",
    "            # backpropragation\n",
    "            loss.backward()\n",
    "            # compute gradient\n",
    "            train_optimizer.step()\n",
    "            # compute train loss\n",
    "            train_loss += loss.item() * anchor_img.size(0)\n",
    "            # calculate distance\n",
    "            distance_positive = (anchor_img - pos_img).pow(2).sum(1)\n",
    "            # compute accuracy\n",
    "            train_accuracy.append(calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same))\n",
    "\n",
    "    return  train_loss, mean(train_accuracy)\n",
    "\n",
    "def test_model(model, test_loader, test_optimizer, test_criterion):\n",
    "    test_loss = 0\n",
    "    test_accuracy = list()\n",
    "    # set to train mode\n",
    "    model.eval()\n",
    "    # enable grad\n",
    "    with torch.no_grad():\n",
    "        # loop though loader\n",
    "        for anchor_img, pos_img, neg_img in test_loader:\n",
    "            # for calculate accuracy\n",
    "            threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "            is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "            # throw img to CUDA\n",
    "            anchor_img, pos_img,neg_img = anchor_img.to(device), pos_img.to(device),neg_img.to(device)\n",
    "            # training model\n",
    "            anchor_img, pos_img, neg_img = model(anchor_img, pos_img,neg_img)\n",
    "            # calculate loss\n",
    "            loss = test_criterion(anchor_img, pos_img, neg_img)\n",
    "            test_optimizer.zero_grad()\n",
    "            # compute train loss\n",
    "            test_loss += loss.item() * anchor_img.size(0)\n",
    "            # calculate distance\n",
    "            distance_positive = (anchor_img - pos_img).pow(2).sum(1)\n",
    "            # compute accuracy\n",
    "            test_accuracy.append(calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same))\n",
    "\n",
    "    return test_loss, mean(test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_epoch = 120\n",
    "# previous_lr = 0\n",
    "# save_weight_path = './weights/triplet_mobilenetwork_120_epochs.pth'\n",
    "# history_train = {\n",
    "#     'train_loss': list(),\n",
    "#     'val_loss': list(),\n",
    "#     'train_acc': list(),\n",
    "#     'val_acc': list()\n",
    "# }\n",
    "# triplet_model.to(device)\n",
    "# for epoch in tqdm(range(num_epoch)):\n",
    "#     # train model\n",
    "#     epoch_train_loss, train_acc = train_model(triplet_model, train_triplet_dataloader, optimizer, criterion)\n",
    "#     # validate model\n",
    "#     epoch_val_loss, val_acc = test_model(triplet_model, val_triplet_dataloader, optimizer, criterion)\n",
    "#     # compute loss\n",
    "#     train_loss = epoch_train_loss / len(train_triplet_dataloader.sampler)\n",
    "#     val_loss = epoch_val_loss / len(val_triplet_dataloader.sampler)\n",
    "#     # update scheduler\n",
    "#     scheduler.step(train_loss)\n",
    "\n",
    "#     # append historical data\n",
    "#     history_train['train_loss'].append(train_loss)\n",
    "#     history_train['val_loss'].append(val_loss)\n",
    "\n",
    "#     history_train['train_acc'].append(train_acc)\n",
    "#     history_train['val_acc'].append(val_acc)\n",
    "\n",
    "#     # get the learning rate\n",
    "#     optim_lr = optimizer.param_groups[0]['lr']\n",
    "#     if (optim_lr < previous_lr) | (optim_lr > previous_lr):\n",
    "#         print('LEARNING RATE HAS CHANGED!')\n",
    "#     # update learning rate to temp variable\n",
    "#     previous_lr = optim_lr\n",
    "#     # observe status\n",
    "#     print(f'Epoch {epoch+1}/{num_epoch},Learning rate: {optim_lr}, train loss: {round(train_loss,5)}, valid loss: {round(val_loss,5)}, train acc: {round(train_acc,5)}, valid acc: {round(val_acc,5)}')\n",
    "\n",
    "# # save model weight\n",
    "# torch.save(triplet_model.state_dict(), save_weight_path)\n",
    "# # clear GPU memory allocated\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1688/3899745927.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'train_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Train Loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhistory_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'Validation loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history_train' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(np.arange(1,len(history_train['train_loss'])+1), history_train['train_loss'],label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(history_train['val_loss'])+1),history_train['val_loss'], label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(history_train['train_acc'])+1), history_train['train_acc'],label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(history_train['val_acc'])+1),history_train['val_acc'], label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(history_train['train_loss'])+1), history_train['train_loss'],label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(history_train['val_loss'])+1),history_train['val_loss'], label= 'Validation loss')\n",
    "plt.plot(np.arange(1,len(history_train['train_acc'])+1), history_train['train_acc'],label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(history_train['val_acc'])+1),history_train['val_acc'], label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69ce6f1bc9c4b4cbbf91db053675fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define transformation for test set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.6),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "cross_val_train_dataset = FaceDataset(df = train_df,num_sample = 9000, transform = train_transform)\n",
    "# cross_val_val_dataset = FaceDataset(df = val_df,num_sample = 5000, transform = train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05bb28c42c1a4889ac99e44ba10cc739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bde3bf7f46d411c9fff61a681d24f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 1/120,Learning rate: 0.001, train loss: 0.75852, valid loss: 0.59034, train acc: 0.43711, val acc: 0.68247\n",
      "Fold: 1/3, Epoch 2/120,Learning rate: 0.001, train loss: 0.60778, valid loss: 0.50149, train acc: 0.63104, val acc: 0.72749\n",
      "Fold: 1/3, Epoch 3/120,Learning rate: 0.001, train loss: 0.54559, valid loss: 0.4663, train acc: 0.67099, val acc: 0.77508\n",
      "Fold: 1/3, Epoch 4/120,Learning rate: 0.001, train loss: 0.50049, valid loss: 0.45728, train acc: 0.7157, val acc: 0.7747\n",
      "Fold: 1/3, Epoch 5/120,Learning rate: 0.001, train loss: 0.48204, valid loss: 0.46392, train acc: 0.72883, val acc: 0.75418\n",
      "Fold: 1/3, Epoch 6/120,Learning rate: 0.001, train loss: 0.44733, valid loss: 0.39774, train acc: 0.75482, val acc: 0.79555\n",
      "Fold: 1/3, Epoch 7/120,Learning rate: 0.001, train loss: 0.4321, valid loss: 0.41404, train acc: 0.76629, val acc: 0.79483\n",
      "Fold: 1/3, Epoch 8/120,Learning rate: 0.001, train loss: 0.41404, valid loss: 0.39268, train acc: 0.78258, val acc: 0.82632\n",
      "Fold: 1/3, Epoch 9/120,Learning rate: 0.001, train loss: 0.41451, valid loss: 0.41896, train acc: 0.78851, val acc: 0.79279\n",
      "Fold: 1/3, Epoch 10/120,Learning rate: 0.001, train loss: 0.38733, valid loss: 0.37071, train acc: 0.80474, val acc: 0.81844\n",
      "Fold: 1/3, Epoch 11/120,Learning rate: 0.001, train loss: 0.3702, valid loss: 0.3524, train acc: 0.81056, val acc: 0.84275\n",
      "Fold: 1/3, Epoch 12/120,Learning rate: 0.001, train loss: 0.3572, valid loss: 0.31751, train acc: 0.81677, val acc: 0.84361\n",
      "Fold: 1/3, Epoch 13/120,Learning rate: 0.001, train loss: 0.35168, valid loss: 0.34475, train acc: 0.81887, val acc: 0.85838\n",
      "Fold: 1/3, Epoch 14/120,Learning rate: 0.001, train loss: 0.34829, valid loss: 0.30294, train acc: 0.82596, val acc: 0.85358\n",
      "Fold: 1/3, Epoch 15/120,Learning rate: 0.001, train loss: 0.33266, valid loss: 0.28368, train acc: 0.84142, val acc: 0.86365\n",
      "Fold: 1/3, Epoch 16/120,Learning rate: 0.001, train loss: 0.35537, valid loss: 0.33679, train acc: 0.82397, val acc: 0.85819\n",
      "Fold: 1/3, Epoch 17/120,Learning rate: 0.001, train loss: 0.33087, valid loss: 0.30112, train acc: 0.83439, val acc: 0.85909\n",
      "Fold: 1/3, Epoch 18/120,Learning rate: 0.001, train loss: 0.32704, valid loss: 0.29371, train acc: 0.83599, val acc: 0.8683\n",
      "Fold: 1/3, Epoch 19/120,Learning rate: 0.001, train loss: 0.3209, valid loss: 0.27866, train acc: 0.84248, val acc: 0.86826\n",
      "Fold: 1/3, Epoch 20/120,Learning rate: 0.001, train loss: 0.31141, valid loss: 0.27917, train acc: 0.84497, val acc: 0.86574\n",
      "Fold: 1/3, Epoch 21/120,Learning rate: 0.001, train loss: 0.30174, valid loss: 0.28257, train acc: 0.85383, val acc: 0.8836\n",
      "Fold: 1/3, Epoch 22/120,Learning rate: 0.001, train loss: 0.3031, valid loss: 0.27413, train acc: 0.85395, val acc: 0.88184\n",
      "Fold: 1/3, Epoch 23/120,Learning rate: 0.001, train loss: 0.29166, valid loss: 0.27992, train acc: 0.8484, val acc: 0.87172\n",
      "Fold: 1/3, Epoch 24/120,Learning rate: 0.001, train loss: 0.28894, valid loss: 0.27004, train acc: 0.86818, val acc: 0.89096\n",
      "Fold: 1/3, Epoch 25/120,Learning rate: 0.001, train loss: 0.28383, valid loss: 0.25629, train acc: 0.86902, val acc: 0.89048\n",
      "Fold: 1/3, Epoch 26/120,Learning rate: 0.001, train loss: 0.27617, valid loss: 0.25781, train acc: 0.87622, val acc: 0.88516\n",
      "Fold: 1/3, Epoch 27/120,Learning rate: 0.001, train loss: 0.29168, valid loss: 0.2578, train acc: 0.86979, val acc: 0.88488\n",
      "Fold: 1/3, Epoch 28/120,Learning rate: 0.001, train loss: 0.29027, valid loss: 0.26833, train acc: 0.86115, val acc: 0.89404\n",
      "Fold: 1/3, Epoch 29/120,Learning rate: 0.001, train loss: 0.28786, valid loss: 0.25363, train acc: 0.86686, val acc: 0.88284\n",
      "Fold: 1/3, Epoch 30/120,Learning rate: 0.001, train loss: 0.27841, valid loss: 0.23504, train acc: 0.87101, val acc: 0.89523\n",
      "Fold: 1/3, Epoch 31/120,Learning rate: 0.001, train loss: 0.27615, valid loss: 0.25507, train acc: 0.87068, val acc: 0.89381\n",
      "Fold: 1/3, Epoch 32/120,Learning rate: 0.001, train loss: 0.26969, valid loss: 0.24843, train acc: 0.87727, val acc: 0.88906\n",
      "Fold: 1/3, Epoch 33/120,Learning rate: 0.001, train loss: 0.26256, valid loss: 0.23258, train acc: 0.88281, val acc: 0.89072\n",
      "Fold: 1/3, Epoch 34/120,Learning rate: 0.001, train loss: 0.2662, valid loss: 0.24538, train acc: 0.8847, val acc: 0.89666\n",
      "Fold: 1/3, Epoch 35/120,Learning rate: 0.001, train loss: 0.25952, valid loss: 0.25034, train acc: 0.88259, val acc: 0.90022\n",
      "Fold: 1/3, Epoch 36/120,Learning rate: 0.001, train loss: 0.26606, valid loss: 0.2392, train acc: 0.88276, val acc: 0.89637\n",
      "Fold: 1/3, Epoch 37/120,Learning rate: 0.001, train loss: 0.23734, valid loss: 0.21516, train acc: 0.89622, val acc: 0.90421\n",
      "Fold: 1/3, Epoch 38/120,Learning rate: 0.001, train loss: 0.25345, valid loss: 0.2618, train acc: 0.8888, val acc: 0.88341\n",
      "Fold: 1/3, Epoch 39/120,Learning rate: 0.001, train loss: 0.24238, valid loss: 0.2238, train acc: 0.894, val acc: 0.90639\n",
      "Fold: 1/3, Epoch 40/120,Learning rate: 0.001, train loss: 0.24755, valid loss: 0.23716, train acc: 0.89572, val acc: 0.9166\n",
      "Fold: 1/3, Epoch 41/120,Learning rate: 0.001, train loss: 0.24329, valid loss: 0.23735, train acc: 0.89245, val acc: 0.91162\n",
      "Fold: 1/3, Epoch 42/120,Learning rate: 0.001, train loss: 0.24147, valid loss: 0.22286, train acc: 0.89977, val acc: 0.90293\n",
      "Fold: 1/3, Epoch 43/120,Learning rate: 0.001, train loss: 0.22835, valid loss: 0.21734, train acc: 0.89866, val acc: 0.9128\n",
      "Fold: 1/3, Epoch 44/120,Learning rate: 0.001, train loss: 0.23597, valid loss: 0.22106, train acc: 0.90647, val acc: 0.92154\n",
      "Fold: 1/3, Epoch 45/120,Learning rate: 0.001, train loss: 0.23146, valid loss: 0.21508, train acc: 0.8975, val acc: 0.90701\n",
      "Fold: 1/3, Epoch 46/120,Learning rate: 0.001, train loss: 0.23514, valid loss: 0.21557, train acc: 0.89977, val acc: 0.91964\n",
      "Fold: 1/3, Epoch 47/120,Learning rate: 0.001, train loss: 0.21581, valid loss: 0.22446, train acc: 0.91068, val acc: 0.91404\n",
      "Fold: 1/3, Epoch 48/120,Learning rate: 0.001, train loss: 0.22517, valid loss: 0.18987, train acc: 0.90885, val acc: 0.92729\n",
      "Fold: 1/3, Epoch 49/120,Learning rate: 0.001, train loss: 0.22013, valid loss: 0.20229, train acc: 0.90647, val acc: 0.9205\n",
      "Fold: 1/3, Epoch 50/120,Learning rate: 0.001, train loss: 0.20829, valid loss: 0.20904, train acc: 0.90891, val acc: 0.91713\n",
      "Fold: 1/3, Epoch 51/120,Learning rate: 0.001, train loss: 0.21557, valid loss: 0.20292, train acc: 0.90775, val acc: 0.91789\n",
      "Fold: 1/3, Epoch 52/120,Learning rate: 0.001, train loss: 0.21923, valid loss: 0.19941, train acc: 0.9083, val acc: 0.92534\n",
      "Fold: 1/3, Epoch 53/120,Learning rate: 0.001, train loss: 0.21697, valid loss: 0.20957, train acc: 0.90481, val acc: 0.91166\n",
      "Fold: 1/3, Epoch 54/120,Learning rate: 0.001, train loss: 0.21993, valid loss: 0.19891, train acc: 0.90719, val acc: 0.92776\n",
      "Fold: 1/3, Epoch 55/120,Learning rate: 0.001, train loss: 0.22331, valid loss: 0.20066, train acc: 0.90725, val acc: 0.91599\n",
      "Fold: 1/3, Epoch 56/120,Learning rate: 0.001, train loss: 0.20486, valid loss: 0.20361, train acc: 0.91899, val acc: 0.9204\n",
      "Fold: 1/3, Epoch 57/120,Learning rate: 0.001, train loss: 0.21175, valid loss: 0.19145, train acc: 0.91279, val acc: 0.93356\n",
      "Fold: 1/3, Epoch 58/120,Learning rate: 0.001, train loss: 0.2171, valid loss: 0.19642, train acc: 0.92043, val acc: 0.92567\n",
      "Fold: 1/3, Epoch 59/120,Learning rate: 0.001, train loss: 0.19952, valid loss: 0.20351, train acc: 0.91905, val acc: 0.92833\n",
      "Fold: 1/3, Epoch 60/120,Learning rate: 0.001, train loss: 0.20973, valid loss: 0.18438, train acc: 0.91705, val acc: 0.9289\n",
      "Fold: 1/3, Epoch 61/120,Learning rate: 0.001, train loss: 0.20889, valid loss: 0.1828, train acc: 0.91545, val acc: 0.929\n",
      "Fold: 1/3, Epoch 62/120,Learning rate: 0.001, train loss: 0.20408, valid loss: 0.19059, train acc: 0.92232, val acc: 0.93104\n",
      "Fold: 1/3, Epoch 63/120,Learning rate: 0.001, train loss: 0.19656, valid loss: 0.18543, train acc: 0.92032, val acc: 0.93232\n",
      "Fold: 1/3, Epoch 64/120,Learning rate: 0.001, train loss: 0.20293, valid loss: 0.18822, train acc: 0.91838, val acc: 0.92544\n",
      "Fold: 1/3, Epoch 65/120,Learning rate: 0.001, train loss: 0.19364, valid loss: 0.17474, train acc: 0.92199, val acc: 0.93774\n",
      "Fold: 1/3, Epoch 66/120,Learning rate: 0.001, train loss: 0.19399, valid loss: 0.18652, train acc: 0.92115, val acc: 0.927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 1/3, Epoch 67/120,Learning rate: 0.001, train loss: 0.19981, valid loss: 0.19686, train acc: 0.91728, val acc: 0.92962\n",
      "Fold: 1/3, Epoch 68/120,Learning rate: 0.001, train loss: 0.18896, valid loss: 0.18322, train acc: 0.92448, val acc: 0.9394\n",
      "Fold: 1/3, Epoch 69/120,Learning rate: 0.001, train loss: 0.18627, valid loss: 0.17459, train acc: 0.92559, val acc: 0.93669\n",
      "Fold: 1/3, Epoch 70/120,Learning rate: 0.001, train loss: 0.19025, valid loss: 0.18701, train acc: 0.92675, val acc: 0.93042\n",
      "Fold: 1/3, Epoch 71/120,Learning rate: 0.001, train loss: 0.19998, valid loss: 0.18356, train acc: 0.9257, val acc: 0.93769\n",
      "Fold: 1/3, Epoch 72/120,Learning rate: 0.001, train loss: 0.19335, valid loss: 0.17404, train acc: 0.92963, val acc: 0.93465\n",
      "Fold: 1/3, Epoch 73/120,Learning rate: 0.001, train loss: 0.18851, valid loss: 0.17884, train acc: 0.93102, val acc: 0.9337\n",
      "Fold: 1/3, Epoch 74/120,Learning rate: 0.001, train loss: 0.19014, valid loss: 0.16895, train acc: 0.92692, val acc: 0.94078\n",
      "Epoch 00075: reducing learning rate of group 0 to 1.0000e-04.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 75/120,Learning rate: 0.0001, train loss: 0.18926, valid loss: 0.17533, train acc: 0.92891, val acc: 0.93973\n",
      "Fold: 1/3, Epoch 76/120,Learning rate: 0.0001, train loss: 0.18077, valid loss: 0.15645, train acc: 0.93551, val acc: 0.95545\n",
      "Fold: 1/3, Epoch 77/120,Learning rate: 0.0001, train loss: 0.17622, valid loss: 0.16007, train acc: 0.94243, val acc: 0.95194\n",
      "Fold: 1/3, Epoch 78/120,Learning rate: 0.0001, train loss: 0.16232, valid loss: 0.14892, train acc: 0.94548, val acc: 0.95265\n",
      "Fold: 1/3, Epoch 79/120,Learning rate: 0.0001, train loss: 0.15796, valid loss: 0.16096, train acc: 0.94781, val acc: 0.95275\n",
      "Fold: 1/3, Epoch 80/120,Learning rate: 0.0001, train loss: 0.15954, valid loss: 0.14523, train acc: 0.94786, val acc: 0.95574\n",
      "Fold: 1/3, Epoch 81/120,Learning rate: 0.0001, train loss: 0.1574, valid loss: 0.14443, train acc: 0.94642, val acc: 0.95208\n",
      "Fold: 1/3, Epoch 82/120,Learning rate: 0.0001, train loss: 0.16225, valid loss: 0.14569, train acc: 0.94869, val acc: 0.95464\n",
      "Fold: 1/3, Epoch 83/120,Learning rate: 0.0001, train loss: 0.16279, valid loss: 0.14369, train acc: 0.95163, val acc: 0.95859\n",
      "Fold: 1/3, Epoch 84/120,Learning rate: 0.0001, train loss: 0.14856, valid loss: 0.14601, train acc: 0.95639, val acc: 0.95464\n",
      "Fold: 1/3, Epoch 85/120,Learning rate: 0.0001, train loss: 0.15954, valid loss: 0.14517, train acc: 0.94681, val acc: 0.95536\n",
      "Fold: 1/3, Epoch 86/120,Learning rate: 0.0001, train loss: 0.14393, valid loss: 0.14377, train acc: 0.95811, val acc: 0.95707\n",
      "Fold: 1/3, Epoch 87/120,Learning rate: 0.0001, train loss: 0.14303, valid loss: 0.14085, train acc: 0.95518, val acc: 0.95925\n",
      "Fold: 1/3, Epoch 88/120,Learning rate: 0.0001, train loss: 0.15161, valid loss: 0.14513, train acc: 0.95501, val acc: 0.95673\n",
      "Fold: 1/3, Epoch 89/120,Learning rate: 0.0001, train loss: 0.15167, valid loss: 0.14368, train acc: 0.95368, val acc: 0.96053\n",
      "Fold: 1/3, Epoch 90/120,Learning rate: 0.0001, train loss: 0.15763, valid loss: 0.14176, train acc: 0.95213, val acc: 0.96167\n",
      "Fold: 1/3, Epoch 91/120,Learning rate: 0.0001, train loss: 0.15955, valid loss: 0.1388, train acc: 0.95246, val acc: 0.96429\n",
      "Fold: 1/3, Epoch 92/120,Learning rate: 0.0001, train loss: 0.15149, valid loss: 0.1427, train acc: 0.95612, val acc: 0.96101\n",
      "Epoch 00093: reducing learning rate of group 0 to 1.0000e-05.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 93/120,Learning rate: 1e-05, train loss: 0.15463, valid loss: 0.12892, train acc: 0.95202, val acc: 0.96201\n",
      "Fold: 1/3, Epoch 94/120,Learning rate: 1e-05, train loss: 0.14307, valid loss: 0.13477, train acc: 0.95656, val acc: 0.96272\n",
      "Fold: 1/3, Epoch 95/120,Learning rate: 1e-05, train loss: 0.15428, valid loss: 0.12944, train acc: 0.95506, val acc: 0.96139\n",
      "Fold: 1/3, Epoch 96/120,Learning rate: 1e-05, train loss: 0.14567, valid loss: 0.1409, train acc: 0.95878, val acc: 0.96134\n",
      "Fold: 1/3, Epoch 97/120,Learning rate: 1e-05, train loss: 0.14632, valid loss: 0.1299, train acc: 0.95977, val acc: 0.96538\n",
      "Fold: 1/3, Epoch 98/120,Learning rate: 1e-05, train loss: 0.14595, valid loss: 0.13724, train acc: 0.95977, val acc: 0.95939\n",
      "Epoch 00099: reducing learning rate of group 0 to 1.0000e-06.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 99/120,Learning rate: 1.0000000000000002e-06, train loss: 0.1455, valid loss: 0.13255, train acc: 0.9549, val acc: 0.96068\n",
      "Fold: 1/3, Epoch 100/120,Learning rate: 1.0000000000000002e-06, train loss: 0.1445, valid loss: 0.14056, train acc: 0.95828, val acc: 0.96253\n",
      "Fold: 1/3, Epoch 101/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14116, valid loss: 0.12795, train acc: 0.95578, val acc: 0.963\n",
      "Fold: 1/3, Epoch 102/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14756, valid loss: 0.13068, train acc: 0.9539, val acc: 0.96329\n",
      "Fold: 1/3, Epoch 103/120,Learning rate: 1.0000000000000002e-06, train loss: 0.15057, valid loss: 0.1398, train acc: 0.9539, val acc: 0.96063\n",
      "Fold: 1/3, Epoch 104/120,Learning rate: 1.0000000000000002e-06, train loss: 0.15009, valid loss: 0.1412, train acc: 0.95595, val acc: 0.96443\n",
      "Fold: 1/3, Epoch 105/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14689, valid loss: 0.14115, train acc: 0.95822, val acc: 0.96205\n",
      "Fold: 1/3, Epoch 106/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14569, valid loss: 0.13168, train acc: 0.95534, val acc: 0.96291\n",
      "Epoch 00107: reducing learning rate of group 0 to 1.0000e-07.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 107/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14772, valid loss: 0.13034, train acc: 0.95623, val acc: 0.96068\n",
      "Fold: 1/3, Epoch 108/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14482, valid loss: 0.13679, train acc: 0.95833, val acc: 0.96376\n",
      "Fold: 1/3, Epoch 109/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15528, valid loss: 0.13955, train acc: 0.95163, val acc: 0.96357\n",
      "Fold: 1/3, Epoch 110/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14994, valid loss: 0.13664, train acc: 0.95972, val acc: 0.96186\n",
      "Fold: 1/3, Epoch 111/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15307, valid loss: 0.13781, train acc: 0.95085, val acc: 0.96505\n",
      "Fold: 1/3, Epoch 112/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15073, valid loss: 0.12728, train acc: 0.95185, val acc: 0.96505\n",
      "Epoch 00113: reducing learning rate of group 0 to 1.0000e-08.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1/3, Epoch 113/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14363, valid loss: 0.13567, train acc: 0.95678, val acc: 0.95963\n",
      "Fold: 1/3, Epoch 114/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14963, valid loss: 0.13712, train acc: 0.9524, val acc: 0.9649\n",
      "Fold: 1/3, Epoch 115/120,Learning rate: 1.0000000000000004e-08, train loss: 0.15063, valid loss: 0.14358, train acc: 0.95257, val acc: 0.96201\n",
      "Fold: 1/3, Epoch 116/120,Learning rate: 1.0000000000000004e-08, train loss: 0.15143, valid loss: 0.13161, train acc: 0.95529, val acc: 0.96576\n",
      "Fold: 1/3, Epoch 117/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14846, valid loss: 0.13114, train acc: 0.95861, val acc: 0.96367\n",
      "Fold: 1/3, Epoch 118/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14397, valid loss: 0.13865, train acc: 0.95673, val acc: 0.96068\n",
      "Fold: 1/3, Epoch 119/120,Learning rate: 1.0000000000000004e-08, train loss: 0.1474, valid loss: 0.13856, train acc: 0.95822, val acc: 0.95973\n",
      "Fold: 1/3, Epoch 120/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14512, valid loss: 0.13418, train acc: 0.95772, val acc: 0.96533\n",
      "Average result 1/3, Avg train loss: 0.23374632793929842 Avg Val loss: 0.21470129382097058 avg train acc: 0.8982989804964538 avg val acc: 0.9147883421985815\n",
      "Fold: 1/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be0aa80d0a0452693b6ee9c5f555774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 1/120,Learning rate: 0.001, train loss: 0.79725, valid loss: 0.65465, train acc: 0.42836, val acc: 0.66024\n",
      "Fold: 2/3, Epoch 2/120,Learning rate: 0.001, train loss: 0.6049, valid loss: 0.54427, train acc: 0.61691, val acc: 0.69139\n",
      "Fold: 2/3, Epoch 3/120,Learning rate: 0.001, train loss: 0.53469, valid loss: 0.5432, train acc: 0.67769, val acc: 0.76368\n",
      "Fold: 2/3, Epoch 4/120,Learning rate: 0.001, train loss: 0.50086, valid loss: 0.50304, train acc: 0.70994, val acc: 0.75527\n",
      "Fold: 2/3, Epoch 5/120,Learning rate: 0.001, train loss: 0.48486, valid loss: 0.4038, train acc: 0.72407, val acc: 0.82456\n",
      "Fold: 2/3, Epoch 6/120,Learning rate: 0.001, train loss: 0.47531, valid loss: 0.41287, train acc: 0.73432, val acc: 0.76567\n",
      "Fold: 2/3, Epoch 7/120,Learning rate: 0.001, train loss: 0.43143, valid loss: 0.38141, train acc: 0.75754, val acc: 0.79939\n",
      "Fold: 2/3, Epoch 8/120,Learning rate: 0.001, train loss: 0.41254, valid loss: 0.39008, train acc: 0.76779, val acc: 0.80015\n",
      "Fold: 2/3, Epoch 9/120,Learning rate: 0.001, train loss: 0.40077, valid loss: 0.36406, train acc: 0.78086, val acc: 0.81188\n",
      "Fold: 2/3, Epoch 10/120,Learning rate: 0.001, train loss: 0.38485, valid loss: 0.35773, train acc: 0.79743, val acc: 0.84437\n",
      "Fold: 2/3, Epoch 11/120,Learning rate: 0.001, train loss: 0.36948, valid loss: 0.31869, train acc: 0.7982, val acc: 0.84356\n",
      "Fold: 2/3, Epoch 12/120,Learning rate: 0.001, train loss: 0.37443, valid loss: 0.3082, train acc: 0.80807, val acc: 0.86066\n",
      "Fold: 2/3, Epoch 13/120,Learning rate: 0.001, train loss: 0.35653, valid loss: 0.31457, train acc: 0.81843, val acc: 0.8542\n",
      "Fold: 2/3, Epoch 14/120,Learning rate: 0.001, train loss: 0.35321, valid loss: 0.32296, train acc: 0.8151, val acc: 0.83468\n",
      "Fold: 2/3, Epoch 15/120,Learning rate: 0.001, train loss: 0.33323, valid loss: 0.31543, train acc: 0.8309, val acc: 0.8713\n",
      "Fold: 2/3, Epoch 16/120,Learning rate: 0.001, train loss: 0.32668, valid loss: 0.30053, train acc: 0.83245, val acc: 0.86916\n",
      "Fold: 2/3, Epoch 17/120,Learning rate: 0.001, train loss: 0.32157, valid loss: 0.28903, train acc: 0.84148, val acc: 0.86569\n",
      "Fold: 2/3, Epoch 18/120,Learning rate: 0.001, train loss: 0.3206, valid loss: 0.28924, train acc: 0.83743, val acc: 0.87585\n",
      "Fold: 2/3, Epoch 19/120,Learning rate: 0.001, train loss: 0.31736, valid loss: 0.29762, train acc: 0.83754, val acc: 0.88554\n",
      "Fold: 2/3, Epoch 20/120,Learning rate: 0.001, train loss: 0.31415, valid loss: 0.29035, train acc: 0.84181, val acc: 0.87718\n",
      "Fold: 2/3, Epoch 21/120,Learning rate: 0.001, train loss: 0.30803, valid loss: 0.2846, train acc: 0.84674, val acc: 0.87899\n",
      "Fold: 2/3, Epoch 22/120,Learning rate: 0.001, train loss: 0.30961, valid loss: 0.26553, train acc: 0.8463, val acc: 0.87794\n",
      "Fold: 2/3, Epoch 23/120,Learning rate: 0.001, train loss: 0.31139, valid loss: 0.28962, train acc: 0.85073, val acc: 0.8816\n",
      "Fold: 2/3, Epoch 24/120,Learning rate: 0.001, train loss: 0.30342, valid loss: 0.24877, train acc: 0.85672, val acc: 0.89875\n",
      "Fold: 2/3, Epoch 25/120,Learning rate: 0.001, train loss: 0.28458, valid loss: 0.25054, train acc: 0.86724, val acc: 0.89537\n",
      "Fold: 2/3, Epoch 26/120,Learning rate: 0.001, train loss: 0.2821, valid loss: 0.23122, train acc: 0.86613, val acc: 0.90293\n",
      "Fold: 2/3, Epoch 27/120,Learning rate: 0.001, train loss: 0.28548, valid loss: 0.26702, train acc: 0.86403, val acc: 0.89115\n",
      "Fold: 2/3, Epoch 28/120,Learning rate: 0.001, train loss: 0.29207, valid loss: 0.23939, train acc: 0.86403, val acc: 0.91831\n",
      "Fold: 2/3, Epoch 29/120,Learning rate: 0.001, train loss: 0.28458, valid loss: 0.25322, train acc: 0.85904, val acc: 0.89153\n",
      "Fold: 2/3, Epoch 30/120,Learning rate: 0.001, train loss: 0.26749, valid loss: 0.24435, train acc: 0.87217, val acc: 0.90658\n",
      "Fold: 2/3, Epoch 31/120,Learning rate: 0.001, train loss: 0.26328, valid loss: 0.23653, train acc: 0.88037, val acc: 0.90207\n",
      "Fold: 2/3, Epoch 32/120,Learning rate: 0.001, train loss: 0.26568, valid loss: 0.27095, train acc: 0.88359, val acc: 0.89371\n",
      "Fold: 2/3, Epoch 33/120,Learning rate: 0.001, train loss: 0.26116, valid loss: 0.24374, train acc: 0.87317, val acc: 0.90549\n",
      "Fold: 2/3, Epoch 34/120,Learning rate: 0.001, train loss: 0.25307, valid loss: 0.22544, train acc: 0.88237, val acc: 0.91556\n",
      "Fold: 2/3, Epoch 35/120,Learning rate: 0.001, train loss: 0.2358, valid loss: 0.21428, train acc: 0.88121, val acc: 0.9195\n",
      "Fold: 2/3, Epoch 36/120,Learning rate: 0.001, train loss: 0.25613, valid loss: 0.2273, train acc: 0.88115, val acc: 0.91314\n",
      "Fold: 2/3, Epoch 37/120,Learning rate: 0.001, train loss: 0.24311, valid loss: 0.24446, train acc: 0.88918, val acc: 0.91271\n",
      "Fold: 2/3, Epoch 38/120,Learning rate: 0.001, train loss: 0.23804, valid loss: 0.22247, train acc: 0.88481, val acc: 0.9158\n",
      "Fold: 2/3, Epoch 39/120,Learning rate: 0.001, train loss: 0.24368, valid loss: 0.21561, train acc: 0.89384, val acc: 0.92097\n",
      "Fold: 2/3, Epoch 40/120,Learning rate: 0.001, train loss: 0.23582, valid loss: 0.21563, train acc: 0.88697, val acc: 0.91466\n",
      "Epoch 00041: reducing learning rate of group 0 to 1.0000e-04.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 41/120,Learning rate: 0.0001, train loss: 0.23898, valid loss: 0.21207, train acc: 0.89738, val acc: 0.92705\n",
      "Fold: 2/3, Epoch 42/120,Learning rate: 0.0001, train loss: 0.21435, valid loss: 0.18726, train acc: 0.90985, val acc: 0.93589\n",
      "Fold: 2/3, Epoch 43/120,Learning rate: 0.0001, train loss: 0.20601, valid loss: 0.18104, train acc: 0.91739, val acc: 0.94073\n",
      "Fold: 2/3, Epoch 44/120,Learning rate: 0.0001, train loss: 0.19628, valid loss: 0.16091, train acc: 0.92481, val acc: 0.9403\n",
      "Fold: 2/3, Epoch 45/120,Learning rate: 0.0001, train loss: 0.20146, valid loss: 0.18413, train acc: 0.92055, val acc: 0.94415\n",
      "Fold: 2/3, Epoch 46/120,Learning rate: 0.0001, train loss: 0.20277, valid loss: 0.18278, train acc: 0.92016, val acc: 0.93764\n",
      "Fold: 2/3, Epoch 47/120,Learning rate: 0.0001, train loss: 0.20387, valid loss: 0.16928, train acc: 0.92431, val acc: 0.94999\n",
      "Fold: 2/3, Epoch 48/120,Learning rate: 0.0001, train loss: 0.19547, valid loss: 0.17349, train acc: 0.92525, val acc: 0.95279\n",
      "Fold: 2/3, Epoch 49/120,Learning rate: 0.0001, train loss: 0.19122, valid loss: 0.15958, train acc: 0.92758, val acc: 0.94771\n",
      "Fold: 2/3, Epoch 50/120,Learning rate: 0.0001, train loss: 0.19874, valid loss: 0.16244, train acc: 0.92769, val acc: 0.95284\n",
      "Fold: 2/3, Epoch 51/120,Learning rate: 0.0001, train loss: 0.18552, valid loss: 0.16628, train acc: 0.9288, val acc: 0.94752\n",
      "Fold: 2/3, Epoch 52/120,Learning rate: 0.0001, train loss: 0.17385, valid loss: 0.16751, train acc: 0.93556, val acc: 0.95137\n",
      "Fold: 2/3, Epoch 53/120,Learning rate: 0.0001, train loss: 0.18619, valid loss: 0.16304, train acc: 0.9329, val acc: 0.95303\n",
      "Fold: 2/3, Epoch 54/120,Learning rate: 0.0001, train loss: 0.17589, valid loss: 0.16156, train acc: 0.93373, val acc: 0.95004\n",
      "Fold: 2/3, Epoch 55/120,Learning rate: 0.0001, train loss: 0.17212, valid loss: 0.16243, train acc: 0.93534, val acc: 0.94804\n",
      "Fold: 2/3, Epoch 56/120,Learning rate: 0.0001, train loss: 0.17709, valid loss: 0.14596, train acc: 0.93434, val acc: 0.95199\n",
      "Fold: 2/3, Epoch 57/120,Learning rate: 0.0001, train loss: 0.17898, valid loss: 0.15735, train acc: 0.93562, val acc: 0.95203\n",
      "Fold: 2/3, Epoch 58/120,Learning rate: 0.0001, train loss: 0.17873, valid loss: 0.16378, train acc: 0.93877, val acc: 0.95089\n",
      "Fold: 2/3, Epoch 59/120,Learning rate: 0.0001, train loss: 0.17066, valid loss: 0.16729, train acc: 0.93778, val acc: 0.95237\n",
      "Fold: 2/3, Epoch 60/120,Learning rate: 0.0001, train loss: 0.17349, valid loss: 0.15692, train acc: 0.94354, val acc: 0.9573\n",
      "Fold: 2/3, Epoch 61/120,Learning rate: 0.0001, train loss: 0.1727, valid loss: 0.15728, train acc: 0.94299, val acc: 0.9527\n",
      "Fold: 2/3, Epoch 62/120,Learning rate: 0.0001, train loss: 0.17743, valid loss: 0.15329, train acc: 0.94116, val acc: 0.95028\n",
      "Fold: 2/3, Epoch 63/120,Learning rate: 0.0001, train loss: 0.17782, valid loss: 0.1527, train acc: 0.94171, val acc: 0.9527\n",
      "Fold: 2/3, Epoch 64/120,Learning rate: 0.0001, train loss: 0.17089, valid loss: 0.14542, train acc: 0.94221, val acc: 0.95502\n",
      "Epoch 00065: reducing learning rate of group 0 to 1.0000e-05.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 65/120,Learning rate: 1e-05, train loss: 0.17203, valid loss: 0.15223, train acc: 0.93739, val acc: 0.95702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 2/3, Epoch 66/120,Learning rate: 1e-05, train loss: 0.16698, valid loss: 0.16797, train acc: 0.93639, val acc: 0.95327\n",
      "Fold: 2/3, Epoch 67/120,Learning rate: 1e-05, train loss: 0.17146, valid loss: 0.15874, train acc: 0.93966, val acc: 0.95597\n",
      "Fold: 2/3, Epoch 68/120,Learning rate: 1e-05, train loss: 0.16675, valid loss: 0.14427, train acc: 0.94498, val acc: 0.95939\n",
      "Fold: 2/3, Epoch 69/120,Learning rate: 1e-05, train loss: 0.17272, valid loss: 0.14144, train acc: 0.94082, val acc: 0.96068\n",
      "Fold: 2/3, Epoch 70/120,Learning rate: 1e-05, train loss: 0.16632, valid loss: 0.14226, train acc: 0.9401, val acc: 0.95868\n",
      "Fold: 2/3, Epoch 71/120,Learning rate: 1e-05, train loss: 0.16995, valid loss: 0.15444, train acc: 0.94138, val acc: 0.9564\n",
      "Fold: 2/3, Epoch 72/120,Learning rate: 1e-05, train loss: 0.17064, valid loss: 0.14837, train acc: 0.94188, val acc: 0.95654\n",
      "Fold: 2/3, Epoch 73/120,Learning rate: 1e-05, train loss: 0.17277, valid loss: 0.15412, train acc: 0.93999, val acc: 0.95232\n",
      "Fold: 2/3, Epoch 74/120,Learning rate: 1e-05, train loss: 0.16612, valid loss: 0.15556, train acc: 0.94343, val acc: 0.95502\n",
      "Fold: 2/3, Epoch 75/120,Learning rate: 1e-05, train loss: 0.165, valid loss: 0.14612, train acc: 0.94648, val acc: 0.95868\n",
      "Fold: 2/3, Epoch 76/120,Learning rate: 1e-05, train loss: 0.17074, valid loss: 0.15562, train acc: 0.94238, val acc: 0.95612\n",
      "Fold: 2/3, Epoch 77/120,Learning rate: 1e-05, train loss: 0.16832, valid loss: 0.15589, train acc: 0.94154, val acc: 0.95474\n",
      "Fold: 2/3, Epoch 78/120,Learning rate: 1e-05, train loss: 0.16296, valid loss: 0.14041, train acc: 0.94448, val acc: 0.95773\n",
      "Fold: 2/3, Epoch 79/120,Learning rate: 1e-05, train loss: 0.16676, valid loss: 0.14881, train acc: 0.94359, val acc: 0.95175\n",
      "Fold: 2/3, Epoch 80/120,Learning rate: 1e-05, train loss: 0.1703, valid loss: 0.14739, train acc: 0.93628, val acc: 0.95659\n",
      "Fold: 2/3, Epoch 81/120,Learning rate: 1e-05, train loss: 0.17357, valid loss: 0.13913, train acc: 0.94359, val acc: 0.95797\n",
      "Fold: 2/3, Epoch 82/120,Learning rate: 1e-05, train loss: 0.16188, valid loss: 0.14917, train acc: 0.9467, val acc: 0.95669\n",
      "Fold: 2/3, Epoch 83/120,Learning rate: 1e-05, train loss: 0.16569, valid loss: 0.15466, train acc: 0.9462, val acc: 0.95806\n",
      "Fold: 2/3, Epoch 84/120,Learning rate: 1e-05, train loss: 0.16752, valid loss: 0.1449, train acc: 0.93717, val acc: 0.96201\n",
      "Fold: 2/3, Epoch 85/120,Learning rate: 1e-05, train loss: 0.17229, valid loss: 0.15456, train acc: 0.94371, val acc: 0.95702\n",
      "Fold: 2/3, Epoch 86/120,Learning rate: 1e-05, train loss: 0.16849, valid loss: 0.14776, train acc: 0.94221, val acc: 0.96367\n",
      "Fold: 2/3, Epoch 87/120,Learning rate: 1e-05, train loss: 0.17629, valid loss: 0.14655, train acc: 0.94188, val acc: 0.95759\n",
      "Epoch 00088: reducing learning rate of group 0 to 1.0000e-06.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 88/120,Learning rate: 1.0000000000000002e-06, train loss: 0.16883, valid loss: 0.15085, train acc: 0.94537, val acc: 0.96139\n",
      "Fold: 2/3, Epoch 89/120,Learning rate: 1.0000000000000002e-06, train loss: 0.17271, valid loss: 0.14867, train acc: 0.93894, val acc: 0.95517\n",
      "Fold: 2/3, Epoch 90/120,Learning rate: 1.0000000000000002e-06, train loss: 0.16709, valid loss: 0.1432, train acc: 0.94066, val acc: 0.95754\n",
      "Fold: 2/3, Epoch 91/120,Learning rate: 1.0000000000000002e-06, train loss: 0.16811, valid loss: 0.13665, train acc: 0.94404, val acc: 0.95802\n",
      "Fold: 2/3, Epoch 92/120,Learning rate: 1.0000000000000002e-06, train loss: 0.16685, valid loss: 0.15215, train acc: 0.94642, val acc: 0.95203\n",
      "Fold: 2/3, Epoch 93/120,Learning rate: 1.0000000000000002e-06, train loss: 0.1672, valid loss: 0.15401, train acc: 0.93972, val acc: 0.96191\n",
      "Epoch 00094: reducing learning rate of group 0 to 1.0000e-07.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 94/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16261, valid loss: 0.14573, train acc: 0.94559, val acc: 0.95227\n",
      "Fold: 2/3, Epoch 95/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16815, valid loss: 0.14221, train acc: 0.94925, val acc: 0.95778\n",
      "Fold: 2/3, Epoch 96/120,Learning rate: 1.0000000000000002e-07, train loss: 0.162, valid loss: 0.14509, train acc: 0.94592, val acc: 0.95797\n",
      "Fold: 2/3, Epoch 97/120,Learning rate: 1.0000000000000002e-07, train loss: 0.17033, valid loss: 0.1502, train acc: 0.94564, val acc: 0.9584\n",
      "Fold: 2/3, Epoch 98/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15683, valid loss: 0.142, train acc: 0.94686, val acc: 0.95564\n",
      "Fold: 2/3, Epoch 99/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16493, valid loss: 0.1474, train acc: 0.94293, val acc: 0.96196\n",
      "Fold: 2/3, Epoch 100/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16591, valid loss: 0.15728, train acc: 0.94099, val acc: 0.96034\n",
      "Fold: 2/3, Epoch 101/120,Learning rate: 1.0000000000000002e-07, train loss: 0.17181, valid loss: 0.13755, train acc: 0.94149, val acc: 0.96837\n",
      "Fold: 2/3, Epoch 102/120,Learning rate: 1.0000000000000002e-07, train loss: 0.17943, valid loss: 0.15235, train acc: 0.93312, val acc: 0.95388\n",
      "Fold: 2/3, Epoch 103/120,Learning rate: 1.0000000000000002e-07, train loss: 0.17128, valid loss: 0.15296, train acc: 0.94021, val acc: 0.95294\n",
      "Fold: 2/3, Epoch 104/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15329, valid loss: 0.15325, train acc: 0.94908, val acc: 0.95369\n",
      "Fold: 2/3, Epoch 105/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16829, valid loss: 0.15487, train acc: 0.9452, val acc: 0.95825\n",
      "Fold: 2/3, Epoch 106/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16399, valid loss: 0.14365, train acc: 0.94625, val acc: 0.95897\n",
      "Fold: 2/3, Epoch 107/120,Learning rate: 1.0000000000000002e-07, train loss: 0.17905, valid loss: 0.15044, train acc: 0.93744, val acc: 0.9603\n",
      "Fold: 2/3, Epoch 108/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16342, valid loss: 0.14978, train acc: 0.9483, val acc: 0.96039\n",
      "Fold: 2/3, Epoch 109/120,Learning rate: 1.0000000000000002e-07, train loss: 0.16573, valid loss: 0.15147, train acc: 0.94515, val acc: 0.95208\n",
      "Epoch 00110: reducing learning rate of group 0 to 1.0000e-08.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 2/3, Epoch 110/120,Learning rate: 1.0000000000000004e-08, train loss: 0.17219, valid loss: 0.14733, train acc: 0.93933, val acc: 0.95251\n",
      "Fold: 2/3, Epoch 111/120,Learning rate: 1.0000000000000004e-08, train loss: 0.15934, valid loss: 0.15063, train acc: 0.94559, val acc: 0.95393\n",
      "Fold: 2/3, Epoch 112/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16859, valid loss: 0.14795, train acc: 0.93805, val acc: 0.96367\n",
      "Fold: 2/3, Epoch 113/120,Learning rate: 1.0000000000000004e-08, train loss: 0.15884, valid loss: 0.15776, train acc: 0.9416, val acc: 0.95787\n",
      "Fold: 2/3, Epoch 114/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16048, valid loss: 0.14653, train acc: 0.94642, val acc: 0.96272\n",
      "Fold: 2/3, Epoch 115/120,Learning rate: 1.0000000000000004e-08, train loss: 0.17053, valid loss: 0.14648, train acc: 0.94542, val acc: 0.95669\n",
      "Fold: 2/3, Epoch 116/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16601, valid loss: 0.15268, train acc: 0.94819, val acc: 0.95569\n",
      "Fold: 2/3, Epoch 117/120,Learning rate: 1.0000000000000004e-08, train loss: 0.1666, valid loss: 0.14264, train acc: 0.94609, val acc: 0.95469\n",
      "Fold: 2/3, Epoch 118/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16234, valid loss: 0.13767, train acc: 0.94299, val acc: 0.96068\n",
      "Fold: 2/3, Epoch 119/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16331, valid loss: 0.14777, train acc: 0.94681, val acc: 0.96324\n",
      "Fold: 2/3, Epoch 120/120,Learning rate: 1.0000000000000004e-08, train loss: 0.16496, valid loss: 0.14033, train acc: 0.94193, val acc: 0.95659\n",
      "Average result 2/3, Avg train loss: 0.23113607435789374 Avg Val loss: 0.20680116024398143 avg train acc: 0.8984065639775414 avg val acc: 0.9232253672745694\n",
      "Fold: 2/3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e26ca7109a64d7992dff9be32baf293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 1/120,Learning rate: 0.001, train loss: 0.77313, valid loss: 0.59661, train acc: 0.43057, val acc: 0.64765\n",
      "Fold: 3/3, Epoch 2/120,Learning rate: 0.001, train loss: 0.59093, valid loss: 0.50584, train acc: 0.64256, val acc: 0.73893\n",
      "Fold: 3/3, Epoch 3/120,Learning rate: 0.001, train loss: 0.55501, valid loss: 0.4815, train acc: 0.67814, val acc: 0.75898\n",
      "Fold: 3/3, Epoch 4/120,Learning rate: 0.001, train loss: 0.50982, valid loss: 0.40733, train acc: 0.7244, val acc: 0.7889\n",
      "Fold: 3/3, Epoch 5/120,Learning rate: 0.001, train loss: 0.48351, valid loss: 0.36403, train acc: 0.73532, val acc: 0.81464\n",
      "Fold: 3/3, Epoch 6/120,Learning rate: 0.001, train loss: 0.46305, valid loss: 0.37232, train acc: 0.73166, val acc: 0.79493\n",
      "Fold: 3/3, Epoch 7/120,Learning rate: 0.001, train loss: 0.43745, valid loss: 0.41352, train acc: 0.75205, val acc: 0.77726\n",
      "Fold: 3/3, Epoch 8/120,Learning rate: 0.001, train loss: 0.41212, valid loss: 0.33543, train acc: 0.76995, val acc: 0.84812\n",
      "Fold: 3/3, Epoch 9/120,Learning rate: 0.001, train loss: 0.38795, valid loss: 0.36493, train acc: 0.78602, val acc: 0.82205\n",
      "Fold: 3/3, Epoch 10/120,Learning rate: 0.001, train loss: 0.39731, valid loss: 0.32659, train acc: 0.79156, val acc: 0.82404\n",
      "Fold: 3/3, Epoch 11/120,Learning rate: 0.001, train loss: 0.38739, valid loss: 0.3352, train acc: 0.80175, val acc: 0.85994\n",
      "Fold: 3/3, Epoch 12/120,Learning rate: 0.001, train loss: 0.36644, valid loss: 0.35969, train acc: 0.8181, val acc: 0.83943\n",
      "Fold: 3/3, Epoch 13/120,Learning rate: 0.001, train loss: 0.35644, valid loss: 0.30773, train acc: 0.82286, val acc: 0.86697\n",
      "Fold: 3/3, Epoch 14/120,Learning rate: 0.001, train loss: 0.36323, valid loss: 0.29921, train acc: 0.8171, val acc: 0.87728\n",
      "Fold: 3/3, Epoch 15/120,Learning rate: 0.001, train loss: 0.33908, valid loss: 0.29081, train acc: 0.84076, val acc: 0.86436\n",
      "Fold: 3/3, Epoch 16/120,Learning rate: 0.001, train loss: 0.34435, valid loss: 0.29588, train acc: 0.82807, val acc: 0.87994\n",
      "Fold: 3/3, Epoch 17/120,Learning rate: 0.001, train loss: 0.32878, valid loss: 0.30008, train acc: 0.83926, val acc: 0.8702\n",
      "Fold: 3/3, Epoch 18/120,Learning rate: 0.001, train loss: 0.32037, valid loss: 0.27249, train acc: 0.84408, val acc: 0.87937\n",
      "Fold: 3/3, Epoch 19/120,Learning rate: 0.001, train loss: 0.31382, valid loss: 0.2575, train acc: 0.84735, val acc: 0.8873\n",
      "Fold: 3/3, Epoch 20/120,Learning rate: 0.001, train loss: 0.31024, valid loss: 0.2572, train acc: 0.84907, val acc: 0.90065\n",
      "Fold: 3/3, Epoch 21/120,Learning rate: 0.001, train loss: 0.29836, valid loss: 0.2798, train acc: 0.85793, val acc: 0.8892\n",
      "Fold: 3/3, Epoch 22/120,Learning rate: 0.001, train loss: 0.30063, valid loss: 0.25277, train acc: 0.85494, val acc: 0.89951\n",
      "Fold: 3/3, Epoch 23/120,Learning rate: 0.001, train loss: 0.30321, valid loss: 0.25754, train acc: 0.8581, val acc: 0.8996\n",
      "Fold: 3/3, Epoch 24/120,Learning rate: 0.001, train loss: 0.29315, valid loss: 0.24132, train acc: 0.85888, val acc: 0.90568\n",
      "Fold: 3/3, Epoch 25/120,Learning rate: 0.001, train loss: 0.29544, valid loss: 0.2583, train acc: 0.85422, val acc: 0.89694\n",
      "Fold: 3/3, Epoch 26/120,Learning rate: 0.001, train loss: 0.29879, valid loss: 0.23194, train acc: 0.86868, val acc: 0.91475\n",
      "Fold: 3/3, Epoch 27/120,Learning rate: 0.001, train loss: 0.27728, valid loss: 0.22934, train acc: 0.87284, val acc: 0.91\n",
      "Fold: 3/3, Epoch 28/120,Learning rate: 0.001, train loss: 0.27383, valid loss: 0.24088, train acc: 0.87832, val acc: 0.91409\n",
      "Fold: 3/3, Epoch 29/120,Learning rate: 0.001, train loss: 0.27036, valid loss: 0.21658, train acc: 0.87289, val acc: 0.90967\n",
      "Fold: 3/3, Epoch 30/120,Learning rate: 0.001, train loss: 0.26627, valid loss: 0.24339, train acc: 0.87522, val acc: 0.91124\n",
      "Fold: 3/3, Epoch 31/120,Learning rate: 0.001, train loss: 0.27, valid loss: 0.24643, train acc: 0.88015, val acc: 0.90801\n",
      "Fold: 3/3, Epoch 32/120,Learning rate: 0.001, train loss: 0.26455, valid loss: 0.239, train acc: 0.88536, val acc: 0.9158\n",
      "Fold: 3/3, Epoch 33/120,Learning rate: 0.001, train loss: 0.27057, valid loss: 0.21921, train acc: 0.88198, val acc: 0.91774\n",
      "Fold: 3/3, Epoch 34/120,Learning rate: 0.001, train loss: 0.25579, valid loss: 0.21303, train acc: 0.88359, val acc: 0.91613\n",
      "Fold: 3/3, Epoch 35/120,Learning rate: 0.001, train loss: 0.25957, valid loss: 0.21097, train acc: 0.88431, val acc: 0.928\n",
      "Fold: 3/3, Epoch 36/120,Learning rate: 0.001, train loss: 0.25602, valid loss: 0.22075, train acc: 0.88409, val acc: 0.92273\n",
      "Fold: 3/3, Epoch 37/120,Learning rate: 0.001, train loss: 0.24938, valid loss: 0.22438, train acc: 0.89229, val acc: 0.92358\n",
      "Fold: 3/3, Epoch 38/120,Learning rate: 0.001, train loss: 0.25181, valid loss: 0.21846, train acc: 0.88813, val acc: 0.92506\n",
      "Fold: 3/3, Epoch 39/120,Learning rate: 0.001, train loss: 0.2408, valid loss: 0.20598, train acc: 0.90226, val acc: 0.92957\n",
      "Fold: 3/3, Epoch 40/120,Learning rate: 0.001, train loss: 0.2377, valid loss: 0.21452, train acc: 0.89905, val acc: 0.92425\n",
      "Fold: 3/3, Epoch 41/120,Learning rate: 0.001, train loss: 0.2423, valid loss: 0.21943, train acc: 0.89306, val acc: 0.92472\n",
      "Fold: 3/3, Epoch 42/120,Learning rate: 0.001, train loss: 0.25167, valid loss: 0.20801, train acc: 0.8909, val acc: 0.91846\n",
      "Fold: 3/3, Epoch 43/120,Learning rate: 0.001, train loss: 0.23088, valid loss: 0.21178, train acc: 0.90132, val acc: 0.92287\n",
      "Fold: 3/3, Epoch 44/120,Learning rate: 0.001, train loss: 0.23564, valid loss: 0.21073, train acc: 0.90775, val acc: 0.92453\n",
      "Fold: 3/3, Epoch 45/120,Learning rate: 0.001, train loss: 0.24317, valid loss: 0.19752, train acc: 0.90038, val acc: 0.92772\n",
      "Fold: 3/3, Epoch 46/120,Learning rate: 0.001, train loss: 0.23417, valid loss: 0.20203, train acc: 0.90193, val acc: 0.92544\n",
      "Fold: 3/3, Epoch 47/120,Learning rate: 0.001, train loss: 0.24664, valid loss: 0.21322, train acc: 0.89273, val acc: 0.91798\n",
      "Fold: 3/3, Epoch 48/120,Learning rate: 0.001, train loss: 0.23407, valid loss: 0.20004, train acc: 0.90354, val acc: 0.92605\n",
      "Fold: 3/3, Epoch 49/120,Learning rate: 0.001, train loss: 0.22395, valid loss: 0.19586, train acc: 0.90996, val acc: 0.93475\n",
      "Fold: 3/3, Epoch 50/120,Learning rate: 0.001, train loss: 0.22518, valid loss: 0.19773, train acc: 0.91113, val acc: 0.9281\n",
      "Fold: 3/3, Epoch 51/120,Learning rate: 0.001, train loss: 0.22076, valid loss: 0.20407, train acc: 0.90619, val acc: 0.9223\n",
      "Fold: 3/3, Epoch 52/120,Learning rate: 0.001, train loss: 0.2237, valid loss: 0.19322, train acc: 0.90891, val acc: 0.93869\n",
      "Fold: 3/3, Epoch 53/120,Learning rate: 0.001, train loss: 0.22384, valid loss: 0.18975, train acc: 0.91218, val acc: 0.9357\n",
      "Fold: 3/3, Epoch 54/120,Learning rate: 0.001, train loss: 0.22868, valid loss: 0.19587, train acc: 0.9006, val acc: 0.9337\n",
      "Fold: 3/3, Epoch 55/120,Learning rate: 0.001, train loss: 0.22429, valid loss: 0.19012, train acc: 0.90697, val acc: 0.94163\n",
      "Fold: 3/3, Epoch 56/120,Learning rate: 0.001, train loss: 0.21955, valid loss: 0.1977, train acc: 0.9083, val acc: 0.93598\n",
      "Fold: 3/3, Epoch 57/120,Learning rate: 0.001, train loss: 0.21426, valid loss: 0.19451, train acc: 0.91755, val acc: 0.93911\n",
      "Fold: 3/3, Epoch 58/120,Learning rate: 0.001, train loss: 0.21351, valid loss: 0.19289, train acc: 0.91639, val acc: 0.93997\n",
      "Fold: 3/3, Epoch 59/120,Learning rate: 0.001, train loss: 0.21427, valid loss: 0.19028, train acc: 0.90908, val acc: 0.94101\n",
      "Fold: 3/3, Epoch 60/120,Learning rate: 0.001, train loss: 0.21294, valid loss: 0.18195, train acc: 0.91439, val acc: 0.93935\n",
      "Fold: 3/3, Epoch 61/120,Learning rate: 0.001, train loss: 0.21375, valid loss: 0.17659, train acc: 0.91872, val acc: 0.93859\n",
      "Fold: 3/3, Epoch 62/120,Learning rate: 0.001, train loss: 0.20124, valid loss: 0.17994, train acc: 0.92093, val acc: 0.93907\n",
      "Fold: 3/3, Epoch 63/120,Learning rate: 0.001, train loss: 0.20865, valid loss: 0.19118, train acc: 0.91539, val acc: 0.94239\n",
      "Fold: 3/3, Epoch 64/120,Learning rate: 0.001, train loss: 0.19667, valid loss: 0.1851, train acc: 0.92681, val acc: 0.93669\n",
      "Fold: 3/3, Epoch 65/120,Learning rate: 0.001, train loss: 0.21319, valid loss: 0.17058, train acc: 0.9262, val acc: 0.95232\n",
      "Fold: 3/3, Epoch 66/120,Learning rate: 0.001, train loss: 0.20847, valid loss: 0.18095, train acc: 0.91717, val acc: 0.94386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 3/3, Epoch 67/120,Learning rate: 0.001, train loss: 0.19439, valid loss: 0.16645, train acc: 0.9273, val acc: 0.95042\n",
      "Fold: 3/3, Epoch 68/120,Learning rate: 0.001, train loss: 0.20338, valid loss: 0.17221, train acc: 0.92348, val acc: 0.94695\n",
      "Fold: 3/3, Epoch 69/120,Learning rate: 0.001, train loss: 0.19323, valid loss: 0.17571, train acc: 0.92681, val acc: 0.95132\n",
      "Fold: 3/3, Epoch 70/120,Learning rate: 0.001, train loss: 0.19151, valid loss: 0.16698, train acc: 0.9288, val acc: 0.95564\n",
      "Fold: 3/3, Epoch 71/120,Learning rate: 0.001, train loss: 0.18774, valid loss: 0.1611, train acc: 0.93318, val acc: 0.95104\n",
      "Fold: 3/3, Epoch 72/120,Learning rate: 0.001, train loss: 0.18507, valid loss: 0.17181, train acc: 0.93196, val acc: 0.94301\n",
      "Fold: 3/3, Epoch 73/120,Learning rate: 0.001, train loss: 0.19495, valid loss: 0.17608, train acc: 0.92758, val acc: 0.94766\n",
      "Fold: 3/3, Epoch 74/120,Learning rate: 0.001, train loss: 0.20198, valid loss: 0.17067, train acc: 0.92027, val acc: 0.94548\n",
      "Fold: 3/3, Epoch 75/120,Learning rate: 0.001, train loss: 0.1959, valid loss: 0.18157, train acc: 0.93229, val acc: 0.94206\n",
      "Fold: 3/3, Epoch 76/120,Learning rate: 0.001, train loss: 0.19497, valid loss: 0.16459, train acc: 0.92548, val acc: 0.94895\n",
      "Fold: 3/3, Epoch 77/120,Learning rate: 0.001, train loss: 0.18876, valid loss: 0.16204, train acc: 0.93706, val acc: 0.94871\n",
      "Epoch 00078: reducing learning rate of group 0 to 1.0000e-04.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 78/120,Learning rate: 0.0001, train loss: 0.18729, valid loss: 0.17004, train acc: 0.93866, val acc: 0.9507\n",
      "Fold: 3/3, Epoch 79/120,Learning rate: 0.0001, train loss: 0.18457, valid loss: 0.15279, train acc: 0.93767, val acc: 0.95673\n",
      "Fold: 3/3, Epoch 80/120,Learning rate: 0.0001, train loss: 0.17945, valid loss: 0.15034, train acc: 0.94299, val acc: 0.965\n",
      "Fold: 3/3, Epoch 81/120,Learning rate: 0.0001, train loss: 0.16116, valid loss: 0.13913, train acc: 0.94947, val acc: 0.96866\n",
      "Fold: 3/3, Epoch 82/120,Learning rate: 0.0001, train loss: 0.16379, valid loss: 0.13668, train acc: 0.94836, val acc: 0.96533\n",
      "Fold: 3/3, Epoch 83/120,Learning rate: 0.0001, train loss: 0.16453, valid loss: 0.14683, train acc: 0.94736, val acc: 0.96657\n",
      "Fold: 3/3, Epoch 84/120,Learning rate: 0.0001, train loss: 0.16633, valid loss: 0.14287, train acc: 0.95412, val acc: 0.96932\n",
      "Fold: 3/3, Epoch 85/120,Learning rate: 0.0001, train loss: 0.15071, valid loss: 0.12903, train acc: 0.95767, val acc: 0.97336\n",
      "Fold: 3/3, Epoch 86/120,Learning rate: 0.0001, train loss: 0.15595, valid loss: 0.13383, train acc: 0.95479, val acc: 0.97364\n",
      "Fold: 3/3, Epoch 87/120,Learning rate: 0.0001, train loss: 0.15103, valid loss: 0.12871, train acc: 0.95634, val acc: 0.96671\n",
      "Fold: 3/3, Epoch 88/120,Learning rate: 0.0001, train loss: 0.15748, valid loss: 0.12867, train acc: 0.95429, val acc: 0.97426\n",
      "Fold: 3/3, Epoch 89/120,Learning rate: 0.0001, train loss: 0.1492, valid loss: 0.12433, train acc: 0.9585, val acc: 0.97022\n",
      "Fold: 3/3, Epoch 90/120,Learning rate: 0.0001, train loss: 0.13834, valid loss: 0.12712, train acc: 0.95922, val acc: 0.96979\n",
      "Fold: 3/3, Epoch 91/120,Learning rate: 0.0001, train loss: 0.15488, valid loss: 0.13309, train acc: 0.95778, val acc: 0.9706\n",
      "Fold: 3/3, Epoch 92/120,Learning rate: 0.0001, train loss: 0.15013, valid loss: 0.12983, train acc: 0.96105, val acc: 0.96728\n",
      "Fold: 3/3, Epoch 93/120,Learning rate: 0.0001, train loss: 0.15474, valid loss: 0.12578, train acc: 0.96171, val acc: 0.97269\n",
      "Fold: 3/3, Epoch 94/120,Learning rate: 0.0001, train loss: 0.15297, valid loss: 0.12945, train acc: 0.95855, val acc: 0.97331\n",
      "Fold: 3/3, Epoch 95/120,Learning rate: 0.0001, train loss: 0.15137, valid loss: 0.13375, train acc: 0.95972, val acc: 0.97336\n",
      "Epoch 00096: reducing learning rate of group 0 to 1.0000e-05.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 96/120,Learning rate: 1e-05, train loss: 0.15186, valid loss: 0.12532, train acc: 0.96094, val acc: 0.97336\n",
      "Fold: 3/3, Epoch 97/120,Learning rate: 1e-05, train loss: 0.15094, valid loss: 0.12496, train acc: 0.95401, val acc: 0.97507\n",
      "Fold: 3/3, Epoch 98/120,Learning rate: 1e-05, train loss: 0.14552, valid loss: 0.12343, train acc: 0.957, val acc: 0.97701\n",
      "Fold: 3/3, Epoch 99/120,Learning rate: 1e-05, train loss: 0.14896, valid loss: 0.12996, train acc: 0.96415, val acc: 0.9744\n",
      "Fold: 3/3, Epoch 100/120,Learning rate: 1e-05, train loss: 0.14849, valid loss: 0.13075, train acc: 0.95855, val acc: 0.97435\n",
      "Fold: 3/3, Epoch 101/120,Learning rate: 1e-05, train loss: 0.14367, valid loss: 0.13588, train acc: 0.95972, val acc: 0.9726\n",
      "Epoch 00102: reducing learning rate of group 0 to 1.0000e-06.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 102/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14499, valid loss: 0.12452, train acc: 0.95944, val acc: 0.97103\n",
      "Fold: 3/3, Epoch 103/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14259, valid loss: 0.12348, train acc: 0.95844, val acc: 0.9753\n",
      "Fold: 3/3, Epoch 104/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14079, valid loss: 0.11769, train acc: 0.96044, val acc: 0.98072\n",
      "Fold: 3/3, Epoch 105/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14724, valid loss: 0.12009, train acc: 0.95817, val acc: 0.97839\n",
      "Fold: 3/3, Epoch 106/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14054, valid loss: 0.12626, train acc: 0.96326, val acc: 0.97364\n",
      "Fold: 3/3, Epoch 107/120,Learning rate: 1.0000000000000002e-06, train loss: 0.14524, valid loss: 0.12866, train acc: 0.95983, val acc: 0.97435\n",
      "Epoch 00108: reducing learning rate of group 0 to 1.0000e-07.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 108/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14559, valid loss: 0.12048, train acc: 0.96188, val acc: 0.97568\n",
      "Fold: 3/3, Epoch 109/120,Learning rate: 1.0000000000000002e-07, train loss: 0.15149, valid loss: 0.131, train acc: 0.95955, val acc: 0.97393\n",
      "Fold: 3/3, Epoch 110/120,Learning rate: 1.0000000000000002e-07, train loss: 0.13808, valid loss: 0.12556, train acc: 0.96476, val acc: 0.9753\n",
      "Fold: 3/3, Epoch 111/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14179, valid loss: 0.12665, train acc: 0.9626, val acc: 0.97374\n",
      "Fold: 3/3, Epoch 112/120,Learning rate: 1.0000000000000002e-07, train loss: 0.1476, valid loss: 0.12601, train acc: 0.9616, val acc: 0.97568\n",
      "Fold: 3/3, Epoch 113/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14131, valid loss: 0.12585, train acc: 0.96121, val acc: 0.97231\n",
      "Fold: 3/3, Epoch 114/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14814, valid loss: 0.12212, train acc: 0.96149, val acc: 0.97701\n",
      "Fold: 3/3, Epoch 115/120,Learning rate: 1.0000000000000002e-07, train loss: 0.14046, valid loss: 0.12424, train acc: 0.96193, val acc: 0.97236\n",
      "Epoch 00116: reducing learning rate of group 0 to 1.0000e-08.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 3/3, Epoch 116/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14475, valid loss: 0.11655, train acc: 0.96011, val acc: 0.97597\n",
      "Fold: 3/3, Epoch 117/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14096, valid loss: 0.12722, train acc: 0.96243, val acc: 0.9697\n",
      "Fold: 3/3, Epoch 118/120,Learning rate: 1.0000000000000004e-08, train loss: 0.13962, valid loss: 0.12847, train acc: 0.96471, val acc: 0.97507\n",
      "Fold: 3/3, Epoch 119/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14137, valid loss: 0.13078, train acc: 0.96288, val acc: 0.97089\n",
      "Fold: 3/3, Epoch 120/120,Learning rate: 1.0000000000000004e-08, train loss: 0.14423, valid loss: 0.12976, train acc: 0.96193, val acc: 0.97374\n",
      "Average result 3/3, Avg train loss: 0.23584497230433754 Avg Val loss: 0.20297259475340446 avg train acc: 0.8992986295803782 avg val acc: 0.9274486290526849\n"
     ]
    }
   ],
   "source": [
    "num_epoch = 120\n",
    "previous_lr = 0\n",
    "history_train = {\n",
    "    'train_loss': dict(),\n",
    "    'val_loss': dict(),\n",
    "    'train_acc': dict(),\n",
    "    'val_acc': dict()\n",
    "}\n",
    "split = 3 \n",
    "k_fold = KFold(n_splits = split, shuffle = True, random_state=seed)\n",
    "batch_size = 64\n",
    "for fold, (train_idx, valid_idx) in tqdm(enumerate(k_fold.split(cross_val_train_dataset)),total = split):\n",
    "\n",
    "    triplet_model = MobileFaceNet()\n",
    "    \n",
    "    triplet_model.to(device)\n",
    "    model_config = {\n",
    "    'margin': 1.2,\n",
    "    'lr': 1e-3,\n",
    "    'patience': 5,\n",
    "    'factor': 0.1,\n",
    "    'min_lr': 1e-10,\n",
    "    'threshold': 1e-3\n",
    "    }\n",
    "    criterion = TripletLoss(margin = model_config['margin'])\n",
    "    optimizer = optim.Adam(triplet_model.parameters(), lr=model_config['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=model_config['patience'], factor=model_config['factor'], min_lr=model_config['min_lr'], threshold=model_config['threshold'], verbose=True)\n",
    "    print(f'Fold: {fold}/{split}')\n",
    "\n",
    "    # the number of epoch in each fold\n",
    "    # Random get the sample from dataset\n",
    "    train_subsampler = torch.utils.data.Subset(cross_val_train_dataset,train_idx)\n",
    "    valid_subsampler = torch.utils.data.Subset(cross_val_train_dataset,valid_idx)\n",
    "\n",
    "    # Load data into dataloader\n",
    "    train_loader = DataLoader(train_subsampler, batch_size = batch_size,  shuffle = True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_subsampler, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "    \n",
    "    hist_epoch_train_loss = list()\n",
    "    hist_epoch_val_loss = list()\n",
    "    hist_epoch_train_acc = list()\n",
    "    hist_epoch_val_acc = list()\n",
    "    \n",
    "    for epoch in tqdm(range(num_epoch)):\n",
    "        # train model\n",
    "        epoch_train_loss, train_acc = train_model(triplet_model, train_loader, optimizer, criterion)\n",
    "        # validate model\n",
    "        epoch_val_loss, val_acc = test_model(triplet_model, valid_loader, optimizer, criterion)\n",
    "        # compute loss\n",
    "        train_loss = epoch_train_loss / len(train_loader.sampler)\n",
    "        val_loss = epoch_val_loss / len(valid_loader.sampler)\n",
    "        # update scheduler\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "        hist_epoch_train_loss.append(train_loss)\n",
    "        hist_epoch_val_loss.append(val_loss)\n",
    "        \n",
    "        hist_epoch_train_acc.append(train_acc)\n",
    "        hist_epoch_val_acc.append(val_acc)\n",
    "\n",
    "        # get the learning rate\n",
    "        optim_lr = optimizer.param_groups[0]['lr']\n",
    "        if (optim_lr < previous_lr) | (optim_lr > previous_lr):\n",
    "            print('LEARNING RATE HAS CHANGED!')\n",
    "        # update learning rate to temp variable\n",
    "        previous_lr = optim_lr\n",
    "        # observe status\n",
    "        print(f'Fold: {fold+1}/{split}, Epoch {epoch+1}/{num_epoch},Learning rate: {optim_lr}, train loss: {round(train_loss,5)}, valid loss: {round(val_loss,5)}, train acc: {round(train_acc,5)}, val acc: {round(val_acc,5)}')\n",
    "    \n",
    "    # append historical data\n",
    "    history_train['train_loss'][fold] = hist_epoch_train_loss\n",
    "    history_train['val_loss'][fold] = hist_epoch_val_loss\n",
    "\n",
    "    history_train['train_acc'][fold] = hist_epoch_train_acc\n",
    "    history_train['val_acc'][fold] = hist_epoch_val_acc\n",
    "    \n",
    "    print(f'Average result {fold+1}/{split}, Avg train loss:', mean(history_train['train_loss'][fold]), 'Avg Val loss:',mean(history_train['val_loss'][fold]),'avg train acc:', mean(history_train['train_acc'][fold]), 'avg val acc:', mean(history_train['val_acc'][fold]) )\n",
    "    # clear GPU memory allocated\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_weight_path = './weights/triplet_mobilenetwork_120_epochs_updated.pth'\n",
    "# torch.save(triplet_model.state_dict(), save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7972512998580933,\n",
       " 0.6049048628807068,\n",
       " 0.5346884415944417,\n",
       " 0.5008646562894186,\n",
       " 0.48485850024223326,\n",
       " 0.4753139651616414,\n",
       " 0.4314347077210744,\n",
       " 0.41254191573460897,\n",
       " 0.4007680857976278,\n",
       " 0.38485235516230265,\n",
       " 0.36948317917188006,\n",
       " 0.3744330619176229,\n",
       " 0.3565329434076945,\n",
       " 0.3532071644862493,\n",
       " 0.33323392264048257,\n",
       " 0.3266823902130127,\n",
       " 0.3215738417307536,\n",
       " 0.3206024248600006,\n",
       " 0.3173646051088969,\n",
       " 0.31414996067682904,\n",
       " 0.3080346000989278,\n",
       " 0.30961345219612124,\n",
       " 0.311389554977417,\n",
       " 0.3034162600040436,\n",
       " 0.284575403769811,\n",
       " 0.2820958143870036,\n",
       " 0.28547751609484356,\n",
       " 0.2920718656380971,\n",
       " 0.2845785083770752,\n",
       " 0.2674882452090581,\n",
       " 0.2632804310321808,\n",
       " 0.26567713912328084,\n",
       " 0.26115509470303855,\n",
       " 0.25306651544570924,\n",
       " 0.23580067678292593,\n",
       " 0.2561314174334208,\n",
       " 0.24310588383674622,\n",
       " 0.23804169209798176,\n",
       " 0.24368229059378307,\n",
       " 0.23581870206197103,\n",
       " 0.23897733688354492,\n",
       " 0.21434970752398172,\n",
       " 0.20601265541712444,\n",
       " 0.19628245202700298,\n",
       " 0.20145717708269756,\n",
       " 0.20277032848199208,\n",
       " 0.203873382349809,\n",
       " 0.1954693075021108,\n",
       " 0.1912205496629079,\n",
       " 0.19874361654122671,\n",
       " 0.18552191774050394,\n",
       " 0.17385432895024616,\n",
       " 0.18618594813346862,\n",
       " 0.17589495066801708,\n",
       " 0.17211672866344452,\n",
       " 0.1770897192955017,\n",
       " 0.17898350334167482,\n",
       " 0.17872606039047242,\n",
       " 0.17065649724006654,\n",
       " 0.17349012231826783,\n",
       " 0.17269612109661103,\n",
       " 0.17743151013056438,\n",
       " 0.177823974053065,\n",
       " 0.17088872146606446,\n",
       " 0.17202600971857707,\n",
       " 0.16698192590475083,\n",
       " 0.17146083589394887,\n",
       " 0.16675472148259482,\n",
       " 0.17271714981396993,\n",
       " 0.16631663330396015,\n",
       " 0.16995150538285572,\n",
       " 0.17064252789815268,\n",
       " 0.1727656416495641,\n",
       " 0.1661248979965846,\n",
       " 0.16500408538182576,\n",
       " 0.1707387176354726,\n",
       " 0.16832050697008769,\n",
       " 0.16295916245381037,\n",
       " 0.16675803017616272,\n",
       " 0.17030263288815817,\n",
       " 0.17356756500403087,\n",
       " 0.16187826255957286,\n",
       " 0.16568647972742717,\n",
       " 0.16751871685187023,\n",
       " 0.17228823598225912,\n",
       " 0.16848875564336777,\n",
       " 0.1762918740908305,\n",
       " 0.16883378553390502,\n",
       " 0.1727059427499771,\n",
       " 0.16709409073988596,\n",
       " 0.16811025404930116,\n",
       " 0.166852414170901,\n",
       " 0.16719900715351105,\n",
       " 0.16260574901103975,\n",
       " 0.1681507780154546,\n",
       " 0.16199916247526805,\n",
       " 0.17032533208529155,\n",
       " 0.15683488925298056,\n",
       " 0.16492982717355092,\n",
       " 0.1659087227980296,\n",
       " 0.1718084099292755,\n",
       " 0.17942577656110129,\n",
       " 0.17127850015958151,\n",
       " 0.15329075209299722,\n",
       " 0.16828692225615183,\n",
       " 0.16398645051320393,\n",
       " 0.17905419007937112,\n",
       " 0.1634244863986969,\n",
       " 0.1657289040486018,\n",
       " 0.17219232575098672,\n",
       " 0.15933552924791972,\n",
       " 0.16859098549683887,\n",
       " 0.15884247835477194,\n",
       " 0.16048020633061727,\n",
       " 0.1705282821257909,\n",
       " 0.16600716372330984,\n",
       " 0.16659555526574452,\n",
       " 0.1623445274035136,\n",
       " 0.16331165643533072,\n",
       " 0.16496102567513785]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_train['train_loss'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
