{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import libaries\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Python version:', sys.version)\n",
    "print('CUDA Available:', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU Name:', torch.cuda.get_device_name())\n",
    "    print('GPU Properties:\\n', torch.cuda.get_device_properties('cuda'))\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95, 0)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Cuda is not available, please use cpu instead\")\n",
    "    device = \"cpu\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 41\n",
    "# Define custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, df = None, num_sample = None, transform = None, num_img_pool = 10):\n",
    "        # set random seed for FaceDataset\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        # create constructors\n",
    "        self.unique_img_name = None\n",
    "        self.data = dict()\n",
    "        self.images = list()\n",
    "        # label to indices\n",
    "        self.label_to_indices = dict()\n",
    "        self.labels = list()\n",
    "        # read csv file\n",
    "        self.df = df\n",
    "        # set the transformation\n",
    "        self.transform = transform\n",
    "        # drop last n row from dataframe\n",
    "        self.df = self.df.head(num_sample)\n",
    "        #get the length of entire dataset\n",
    "        self.len_ = len(self.df)\n",
    "        # load imgs\n",
    "        self.load_imgs(self.df, num_imgs = num_img_pool, max = num_sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "\n",
    "    # get each pair of images -> 1: same identity, 0: different identity\n",
    "    # if index is even -> same pair\n",
    "    # if index is odd -> random identity\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img = self.images[idx]\n",
    "        anchor_label = self.labels[idx]\n",
    "\n",
    "        pos_idx = np.random.choice(np.arange(len(self.images))[self.labels == anchor_label])\n",
    "        neg_idx = np.random.choice(np.arange(len(self.images))[self.labels != anchor_label])\n",
    "\n",
    "        pos_img = self.images[pos_idx]\n",
    "        neg_img = self.images[neg_idx]\n",
    "\n",
    "        pos_label = self.labels[pos_idx]\n",
    "        neg_label = self.labels[neg_idx]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            anchor_img = img_to_tensor(anchor_img)\n",
    "            pos_img = img_to_tensor(pos_img)\n",
    "            neg_img = img_to_tensor(neg_img)\n",
    "        else:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            pos_img = self.transform(pos_img)\n",
    "            neg_img = self.transform(neg_img)\n",
    "\n",
    "        return anchor_img, pos_img, neg_img\n",
    "\n",
    "    # load imgs from pandas to memory and define the maximum number of images\n",
    "    def load_imgs(self, df, num_imgs, max):\n",
    "        # iterate thought each row\n",
    "        for i, row in tqdm(df.iterrows(), total = max):\n",
    "            # get identity of each row\n",
    "            row_identity = row['identity']\n",
    "            # append each identity to numberical value\n",
    "            self.label_to_indices[int(row_identity)] = i\n",
    "            count_img = 0\n",
    "            # loop imgs in each identity\n",
    "            for img_name in row['path']:\n",
    "                if count_img > num_imgs:\n",
    "                    break\n",
    "                # concatenate the directoru and image name\n",
    "                # path_to_image = self.dir+img_name\n",
    "                path_to_image = img_name\n",
    "                # open image and convert to RGB\n",
    "                img = Image.open(path_to_image).convert('RGB')\n",
    "\n",
    "                self.images.append(img)\n",
    "                self.labels.append(i)\n",
    "                count_img += 1\n",
    "            # print('Added img '+ str(row_identity))\n",
    "        self.labels = np.array(self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_df = pd.read_csv('./digiface_csv_files/digi_all.csv')\n",
    "ds_df = ds_df.groupby('identity')['path'].apply(list).reset_index()\n",
    "ds_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# splitting each dataset\n",
    "train_df, eval_df = train_test_split(ds_df, test_size= 0.3, shuffle = True, random_state = seed)\n",
    "val_df, test_df = train_test_split(eval_df, test_size = 0.4, shuffle = True, random_state = seed)\n",
    "\n",
    "# print to check size of each dataset\n",
    "print(f'Train Size: {len(train_df)}')\n",
    "print(f'Val Size: {len(val_df)}')\n",
    "print(f'Test Size: {len(test_df)}')\n",
    "\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image size\n",
    "img_size = 112\n",
    "\n",
    "# define transformation for test set\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.6),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# define transformation for validation set\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# define batch size\n",
    "# train_batch_size = 64\n",
    "# val_batch_size = 64\n",
    "# print('------------Started Loading Train Set------------')\n",
    "# # create dataloader for train set\n",
    "# train_triplet_dataset = FaceDataset(df = train_df, num_sample = 6000, transform = train_transform)\n",
    "train_triplet_dataloader = DataLoader(train_triplet_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "# print('Total Train Set: ', train_triplet_dataset.__len__())\n",
    "# print('-----------Finished Loading Train Set------------')\n",
    "#\n",
    "# print('\\n')\n",
    "#\n",
    "# print('------------Started Loading Validation Set------------')\n",
    "# # create dataloader for validation set\n",
    "# val_triplet_dataset = FaceDataset(df = val_df,num_sample = 3000, transform = val_transform)\n",
    "# val_triplet_dataloader = DataLoader(val_triplet_dataset, batch_size=val_batch_size, shuffle=True, pin_memory=True)\n",
    "# print('Total Train Set: ', val_triplet_dataset.__len__())\n",
    "# print('-----------Finished Loading Validation Set------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, ReLU6, Sigmoid, Dropout2d\\\n",
    "    ,Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class h_sigmoid(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
    "        self.fc = Sequential(\n",
    "            Linear(channel, channel // reduction),\n",
    "            ReLU(inplace=True),\n",
    "            Linear(channel // reduction, channel),\n",
    "            h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class PermutationBlock(Module):\n",
    "    def __init__(self, groups):\n",
    "        super(PermutationBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        G = self.groups\n",
    "        output = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n",
    "        return output\n",
    "\n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class LinearScheduler(nn.Module):\n",
    "    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n",
    "        super(LinearScheduler, self).__init__()\n",
    "        self.dropblock = dropblock\n",
    "        self.i = 0\n",
    "        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=nr_steps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropblock(x)\n",
    "\n",
    "    def step(self):\n",
    "        if self.i < len(self.drop_values):\n",
    "            self.dropblock.drop_prob = self.drop_values[self.i]\n",
    "\n",
    "        self.i += 1\n",
    "\n",
    "class DropBlock2D(nn.Module):\n",
    "    r\"\"\"Randomly zeroes 2D spatial blocks of the input tensor.\n",
    "    As described in the paper\n",
    "    `DropBlock: A regularization method for convolutional networks`_ ,\n",
    "    dropping whole blocks of feature map allows to remove semantic\n",
    "    information as compared to regular dropout.\n",
    "    Args:\n",
    "        drop_prob (float): probability of an element to be dropped.\n",
    "        block_size (int): size of the block to drop\n",
    "    Shape:\n",
    "        - Input: `(N, C, H, W)`\n",
    "        - Output: `(N, C, H, W)`\n",
    "    .. _DropBlock: A regularization method for convolutional networks:\n",
    "       https://arxiv.org/abs/1810.12890\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, drop_prob, block_size):\n",
    "        super(DropBlock2D, self).__init__()\n",
    "\n",
    "        self.drop_prob = drop_prob\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # shape: (bsize, channels, height, width)\n",
    "\n",
    "        assert x.dim() == 4, \\\n",
    "            \"Expected input with 4 dimensions (bsize, channels, height, width)\"\n",
    "\n",
    "        if not self.training or self.drop_prob == 0.:\n",
    "            return x\n",
    "        else:\n",
    "            # get gamma value\n",
    "            gamma = self._compute_gamma(x)\n",
    "\n",
    "            # sample mask\n",
    "            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n",
    "\n",
    "            # place mask on input device\n",
    "            mask = mask.to(x.device)\n",
    "\n",
    "            # compute block mask\n",
    "            block_mask = self._compute_block_mask(mask)\n",
    "\n",
    "            # apply block mask\n",
    "            out = x * block_mask[:, None, :, :]\n",
    "\n",
    "            # scale output\n",
    "            out = out * block_mask.numel() / block_mask.sum()\n",
    "\n",
    "            return out\n",
    "\n",
    "    def _compute_block_mask(self, mask):\n",
    "        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n",
    "                                  kernel_size=(self.block_size, self.block_size),\n",
    "                                  stride=(1, 1),\n",
    "                                  padding=self.block_size // 2)\n",
    "\n",
    "        if self.block_size % 2 == 0:\n",
    "            block_mask = block_mask[:, :, :-1, :-1]\n",
    "\n",
    "        block_mask = 1 - block_mask.squeeze(1)\n",
    "\n",
    "        return block_mask\n",
    "\n",
    "    def _compute_gamma(self, x):\n",
    "        return self.drop_prob / (self.block_size ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size=512):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "        self.dropblock = DropBlock2D(block_size=3, drop_prob=0.3)\n",
    "        self.dropout = Dropout2d(0.3)\n",
    "\n",
    "    def forward_once(self,x):\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.conv2_dw(out)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "\n",
    "        out = self.conv_34(out)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "\n",
    "        # out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "\n",
    "        out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "\n",
    "        out = self.dropblock(out)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        return l2_norm(out)\n",
    "\n",
    "    def forward(self, anchor_img, positive_img, negative_img):\n",
    "        anchor = self.forward_once(anchor_img)\n",
    "        positive = self.forward_once(positive_img)\n",
    "        negative = self.forward_once(negative_img)\n",
    "        return anchor, positive, negative\n",
    "\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        # calculate Euclidean's distance\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)\n",
    "        # distance_positive = F.pairwise_distance(anchor, positive).view((-1, 1))\n",
    "        # distance_negative = F.pairwise_distance(anchor, negative).view((-1, 1))\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return torch.mean(losses)\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "\n",
    "    acc = float(tp + tn) / dist.size\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = MobileFaceNet()\n",
    "print(summary(triplet_model, input_size=[(32,3,112,112),(32,3,112,112),(32,3,112,112)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'margin': 1.2,\n",
    "    'lr': 1e-3,\n",
    "    'patience': 5,\n",
    "    'factor': 0.1,\n",
    "    'min_lr': 1e-10,\n",
    "    'threshold': 1e-3\n",
    "}\n",
    "# criterion = TripletLoss(margin = model_config['margin'])\n",
    "# optimizer = optim.Adam(triplet_model.parameters(), lr=model_config['lr'])\n",
    "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=model_config['patience'], factor=model_config['factor'], min_lr=model_config['min_lr'], threshold=model_config['threshold'], verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_dataset = FaceDataset(df = train_df, num_sample= 9000, transform=train_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63af0aacab8444eda60fe44026ad8e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold: 0/5\n",
      "train sample: 141\n",
      "val sample: 29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75bb4dd635b74741a36d281e1be19625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1, Epoch 1 / 120, learning rate: 0.001, train loss: 1.05152,train acc: 0.08528\n",
      "Fold: 1, Epoch 2 / 120, learning rate: 0.001, train loss: 1.05262,train acc: 0.08561\n",
      "Fold: 1, Epoch 3 / 120, learning rate: 0.001, train loss: 1.05765,train acc: 0.08423\n",
      "Fold: 1, Epoch 4 / 120, learning rate: 0.001, train loss: 1.05833,train acc: 0.08311\n",
      "Fold: 1, Epoch 5 / 120, learning rate: 0.001, train loss: 1.05128,train acc: 0.08298\n",
      "Fold: 1, Epoch 6 / 120, learning rate: 0.001, train loss: 1.05765,train acc: 0.08214\n",
      "Fold: 1, Epoch 7 / 120, learning rate: 0.001, train loss: 1.05054,train acc: 0.08172\n",
      "Epoch 00007: reducing learning rate of group 0 to 1.0000e-04.\n",
      "LEARNING RATE HAS CHANGED!\n",
      "Fold: 1, Epoch 8 / 120, learning rate: 0.0001, train loss: 1.05211,train acc: 0.08204\n",
      "Fold: 1, Epoch 9 / 120, learning rate: 0.0001, train loss: 1.04894,train acc: 0.08205\n",
      "Fold: 1, Epoch 10 / 120, learning rate: 0.0001, train loss: 1.05559,train acc: 0.08202\n",
      "Fold: 1, Epoch 11 / 120, learning rate: 0.0001, train loss: 1.05449,train acc: 0.08203\n",
      "Fold: 1, Epoch 12 / 120, learning rate: 0.0001, train loss: 1.05417,train acc: 0.08208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6668/4083722709.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[1;31m# loop though each item in dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0manchor_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneg_img\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mthreshold_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_config\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'margin'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m                 \u001b[1;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 681\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    683\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    719\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 721\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    722\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    723\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6668/767688908.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[0manchor_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manchor_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m             \u001b[0mpos_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpos_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m             \u001b[0mneg_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mneg_img\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1131\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    347\u001b[0m             \u001b[0mPIL\u001b[0m \u001b[0mImage\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRescaled\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \"\"\"\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\transforms\\functional.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    428\u001b[0m             \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Anti-alias option is always applied for PIL Image input. Argument antialias is ignored.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m         \u001b[0mpil_interpolation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpil_modes_mapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF_pil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpil_interpolation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mF_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minterpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mantialias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mantialias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    280\u001b[0m                 \u001b[1;34m\"i.e. size should be an int or a sequence of length 1 in torchscript mode.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m             )\n\u001b[1;32m--> 282\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\pie31\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mresize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   2080\u001b[0m                 )\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2082\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2083\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2084\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from statistics import mean\n",
    "\n",
    "img_size = 112\n",
    "split = 5\n",
    "batch_size = 64\n",
    "cv_epoch = 120\n",
    "\n",
    "k_fold = KFold(n_splits = 5, shuffle = True, random_state=seed)\n",
    "\n",
    "history_cv = {\n",
    "    'cv_train_loss': dict(),\n",
    "    'cv_val_loss': dict(),\n",
    "    'cv_train_acc': dict(),\n",
    "    'cv_val_acc': dict()\n",
    "}\n",
    "\n",
    "# fold level\n",
    "for fold, (train_idx, valid_idx) in tqdm(enumerate(k_fold.split(cross_val_dataset)),total = split):\n",
    "\n",
    "    print(f'Fold: {fold}/{split}')\n",
    "\n",
    "    # the number of epoch in each fold\n",
    "    # Random get the sample from dataset\n",
    "    train_subsampler = torch.utils.data.Subset(cross_val_dataset,train_idx)\n",
    "    valid_subsampler = torch.utils.data.Subset(cross_val_dataset,valid_idx)\n",
    "\n",
    "    # Load data into dataloader\n",
    "    train_loader = DataLoader(cross_val_dataset, batch_size = batch_size,  shuffle = True,pin_memory=True)\n",
    "    valid_loader = DataLoader(valid_subsampler, batch_size = batch_size, shuffle = True, pin_memory=True)\n",
    "\n",
    "    # declare model\n",
    "    triplet_cv_model = MobileFaceNet()\n",
    "    triplet_cv_model.to(device)\n",
    "\n",
    "    # loss function\n",
    "    criterion = TripletLoss(margin = model_config['margin'])\n",
    "    # optimizer function\n",
    "    optimizer = optim.Adam(triplet_cv_model.parameters(), lr=model_config['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=model_config['patience'], factor=model_config['factor'], min_lr=model_config['min_lr'], threshold=model_config['threshold'], verbose=True)\n",
    "\n",
    "    epoch_train_loss = list()\n",
    "    epoch_val_loss = list()\n",
    "\n",
    "    epoch_train_acc = list()\n",
    "    epoch_val_acc = list()\n",
    "    print(f'train sample: {len(train_loader)}')\n",
    "    print(f'val sample: {len(valid_loader)}')\n",
    "    previous_lr = 0\n",
    "\n",
    "    # epoch level\n",
    "    for epoch in tqdm(range(cv_epoch)):\n",
    "\n",
    "        cv_train_loss = 0\n",
    "        cv_val_loss = 0\n",
    "\n",
    "        triplet_cv_model.train()\n",
    "        triplet_model.train()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # loop though each item in dataloader\n",
    "            for anchor_img, pos_img, neg_img in train_loader:\n",
    "\n",
    "                threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "                is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "                # throw img to compute in the device\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                pos_img = pos_img.to(device)\n",
    "                neg_img = neg_img.to(device)\n",
    "                # clear gradient to prevent gradient vanish\n",
    "                optimizer.zero_grad()\n",
    "                # train model\n",
    "                output1, output2, output3 = triplet_model(anchor_img, pos_img, neg_img)\n",
    "\n",
    "                # cal distance positive\n",
    "                distance_positive = (output1 - output2).pow(2).sum(1)\n",
    "                # compute loss from criterion\n",
    "                loss = criterion(output1, output2, output3)\n",
    "                # backward propagate\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "                # append train loss to epoch_train_loss\n",
    "                cv_train_loss += loss.item() * anchor_img.size(0)\n",
    "\n",
    "                epoch_train_acc = np.append(epoch_train_acc,[calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same)])\n",
    "\n",
    "        cv_current_train_loss = cv_train_loss /  len(train_loader.sampler)\n",
    "\n",
    "        epoch_train_loss.append(cv_current_train_loss)\n",
    "        # get learning rate from model\n",
    "        optim_lr = optimizer.param_groups[0]['lr']\n",
    "        if (optim_lr < previous_lr) | (optim_lr > previous_lr):\n",
    "            print('LEARNING RATE HAS CHANGED!')\n",
    "        previous_lr = optim_lr\n",
    "        print(f'Fold: {fold+1}, Epoch {epoch+1} / {cv_epoch}, learning rate: {optim_lr}, train loss: {round(cv_current_train_loss,5)},train acc: {round(epoch_train_acc.mean(),5)}')\n",
    "\n",
    "        scheduler.step(cv_current_train_loss)\n",
    "\n",
    "    history_cv['cv_train_loss'][fold] = epoch_train_loss\n",
    "    history_cv['cv_val_loss'][fold] = epoch_val_loss\n",
    "\n",
    "    history_cv['cv_train_acc'][fold] = epoch_train_acc.mean()\n",
    "    history_cv['cv_val_acc'][fold] = epoch_val_acc.mean()\n",
    "    print(f'------------------------Fold: {fold+1}/{split}--------------------------------')\n",
    "    print(f'avg train loss: {round(mean(epoch_train_loss),5)}, avg val loss: {round(mean(epoch_val_loss),5)}, train acc: {round(epoch_train_acc.mean(),5)}, val acc: {round(epoch_val_acc.mean(),5)}')\n",
    "    print('------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(load_weight_pth = None, save_weight_pth = None, num_epochs = 200 , load_weight = False):\n",
    "    load_weight_path  = './weights/triplet_mobilenetwork_100_epochs_.pth' if (load_weight_pth == None) else load_weight_pth\n",
    "    save_weight_path = './weights/triplet_mobilenetwork_200_epochs_.pth' if (load_weight_pth == None) else save_weight_pth\n",
    "\n",
    "    # throw model to compute in the device\n",
    "    triplet_model.to(device=device)\n",
    "    total_train_loss = list()\n",
    "    total_val_loss = list()\n",
    "\n",
    "    total_train_acc = list()\n",
    "    total_val_acc = list()\n",
    "\n",
    "    best_val_loss = 0\n",
    "    best_train_loss = 0\n",
    "    previous_lr = 0\n",
    "\n",
    "    # load weight\n",
    "    if bool(load_weight_path) & load_weight:\n",
    "        triplet_model.load_state_dict(torch.load(load_weight_path))\n",
    "        print('Congratulations, Weight has been loaded!')\n",
    "\n",
    "    # iterate though each epoch\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print(f'Epoch:{epoch+1}/{num_epochs}')\n",
    "        # epoch_train_loss = list()\n",
    "        epoch_train_acc = np.array([])\n",
    "        # epoch_val_loss = list()\n",
    "        epoch_val_acc = np.array([])\n",
    "\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "\n",
    "        # switch to train mode\n",
    "        triplet_model.train()\n",
    "        with torch.set_grad_enabled(True):\n",
    "            # loop though each item in dataloader\n",
    "            for anchor_img, pos_img, neg_img in tqdm(train_triplet_dataloader):\n",
    "\n",
    "                threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "                is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "                # throw img to compute in the device\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                pos_img = pos_img.to(device)\n",
    "                neg_img = neg_img.to(device)\n",
    "                # clear gradient to prevent gradient vanish\n",
    "                optimizer.zero_grad()\n",
    "                # train model\n",
    "                output1, output2, output3 = triplet_model(anchor_img, pos_img, neg_img)\n",
    "\n",
    "                # cal distance positive\n",
    "                distance_positive = (output1 - output2).pow(2).sum(1)\n",
    "                # compute loss from criterion\n",
    "                loss = criterion(output1, output2, output3)\n",
    "                # backward propagate\n",
    "                loss.backward()\n",
    "                # update parameters\n",
    "                optimizer.step()\n",
    "                # append train loss to epoch_train_loss\n",
    "                train_loss += loss.item() * anchor_img.size(0)\n",
    "\n",
    "                epoch_train_acc = np.append(epoch_train_acc,[calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same)])\n",
    "\n",
    "\n",
    "\n",
    "        # switch mode to eval\n",
    "        triplet_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for anchor_img, pos_img, neg_img in val_triplet_dataloader:\n",
    "\n",
    "                threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "                is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "                # throw img to compute in the device\n",
    "                anchor_img = anchor_img.to(device)\n",
    "                pos_img = pos_img.to(device)\n",
    "                neg_img = neg_img.to(device)\n",
    "                # clear gradient to prevent gradient vanish\n",
    "                optimizer.zero_grad()\n",
    "                # compute the out in eval mode\n",
    "                output1, output2, output3 = triplet_model(anchor_img, pos_img, neg_img)\n",
    "                # cal distance positive\n",
    "                distance_positive = (output1 - output2).pow(2).sum(1)\n",
    "                # compute loss in eval mode\n",
    "                loss = criterion(output1, output2, output3)\n",
    "                # calculate loss for each feature\n",
    "                val_loss += loss.item() * anchor_img.size(0)\n",
    "\n",
    "                # epoch_val_acc.append(calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same)[0])\n",
    "                epoch_val_acc = np.append(epoch_val_acc,[calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same)])\n",
    "\n",
    "        # calculate loss\n",
    "        current_train_loss = train_loss / len(train_triplet_dataloader.sampler)\n",
    "        total_train_loss.append(current_train_loss)\n",
    "\n",
    "        # calculate loss for each batch\n",
    "        current_val_loss = val_loss / len(val_triplet_dataloader.sampler)\n",
    "        total_val_loss.append(current_val_loss)\n",
    "\n",
    "        total_train_acc.append(epoch_train_acc.mean())\n",
    "        total_val_acc.append(epoch_val_acc.mean())\n",
    "        # get learning rate from model\n",
    "        optim_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # append current validation loss to list\n",
    "        if (best_val_loss == 0) | (best_train_loss == 0):\n",
    "            best_val_loss = current_val_loss\n",
    "            best_train_loss = current_train_loss\n",
    "\n",
    "        # find the best val loss\n",
    "        if best_val_loss >= current_val_loss:\n",
    "            best_val_loss = current_val_loss\n",
    "\n",
    "        # find the best train loss\n",
    "        if best_train_loss >= current_train_loss:\n",
    "            best_train_loss = current_train_loss\n",
    "\n",
    "        print(f'train loss: {current_train_loss}')\n",
    "        print(f'val loss: {current_val_loss}')\n",
    "        print(f'Learning rate: {optim_lr:.8f}')\n",
    "        print(f'acc train: {epoch_train_acc.mean()}')\n",
    "        print(f'acc val: {epoch_val_acc.mean()}')\n",
    "        if (optim_lr < previous_lr) | (optim_lr > previous_lr):\n",
    "            print('LEARNING RATE HAS CHANGED!')\n",
    "        print('-------------------------------------------------------------')\n",
    "\n",
    "        # replace the previous lr with the current lr\n",
    "        previous_lr = optim_lr\n",
    "        scheduler.step(current_train_loss)\n",
    "\n",
    "    print('Best Validation loss',best_val_loss)\n",
    "    print('Best Train loss', best_train_loss)\n",
    "\n",
    "    # save model weights\n",
    "    torch.save(triplet_model.state_dict(), save_weight_path)\n",
    "    return total_train_loss, total_val_loss, total_train_acc, total_val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_train_loss, total_val_loss, total_train_acc, total_val_acc = train_nn(num_epochs=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(total_train_loss)+1), total_train_loss,label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(total_val_loss)+1),total_val_loss, label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(total_train_acc)+1), total_train_acc, label = 'Train Acc')\n",
    "plt.plot(np.arange(1,len(total_val_acc)+1), total_val_acc, label = 'Validation Acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(total_train_acc)+1), total_train_acc, label = 'Train Acc')\n",
    "plt.plot(np.arange(1,len(total_val_acc)+1), total_val_acc, label = 'Validation Acc')\n",
    "plt.plot(np.arange(1,len(total_train_loss)+1), total_train_loss,label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(total_val_loss)+1),total_val_loss, label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size for testing\n",
    "test_batch_size = 64\n",
    "# create dataloader for validation set\n",
    "test_triplet_dataset = FaceDataset(df = val_df, num_sample = 3000)\n",
    "test_triplet_dataloader = DataLoader(test_triplet_dataset, batch_size=test_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path\n",
    "weights_path = './weights/triplet_mobilenetwork_200_epochs_.pth'\n",
    "\n",
    "triplet_model.to(device)\n",
    "# load model weights\n",
    "triplet_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "overall_acc = list()\n",
    "\n",
    "# set mode for testing\n",
    "triplet_model.eval()\n",
    "with torch.no_grad():\n",
    "    for anchor_img, pos_img, neg_img in tqdm(test_triplet_dataloader):\n",
    "\n",
    "        threshold_list = np.full(anchor_img.shape[0], model_config['margin'])\n",
    "        is_same = np.full(anchor_img.shape[0], 1)\n",
    "\n",
    "        pos_dist_list = list()\n",
    "        model_output = list()\n",
    "\n",
    "        anchor_img = anchor_img.to(device)\n",
    "        pos_img = pos_img.to(device)\n",
    "        neg_img = neg_img.to(device)\n",
    "        # compute the out in eval mode\n",
    "        anchor_img, pos_img, neg_img = triplet_model(anchor_img, pos_img, neg_img)\n",
    "        # compute loss in eval mode\n",
    "        loss = criterion(anchor_img, pos_img, neg_img)\n",
    "\n",
    "        distance_positive = (anchor_img - pos_img).pow(2).sum(1)\n",
    "        distance_negative = (anchor_img - neg_img).pow(2).sum(1)\n",
    "        # print((distance_negative-distance_positive>0).sum()*1.0 / distance_negative.size()[0])\n",
    "        # print(distance_positive.cpu().data.numpy())\n",
    "        acc = calculate_accuracy(threshold_list,distance_positive.cpu().data.numpy(),is_same)\n",
    "        overall_acc.append(acc)\n",
    "\n",
    "print(f'Train Accuracy: {np.array(overall_acc).mean()}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
