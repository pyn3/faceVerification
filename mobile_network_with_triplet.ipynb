{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0cefa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libaries\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import sys\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022d45fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.9.0 (tags/v3.9.0:9cf6752, Oct  5 2020, 15:34:40) [MSC v.1927 64 bit (AMD64)]\n",
      "CUDA Available: True\n",
      "GPU Name: NVIDIA GeForce RTX 2070\n",
      "GPU Properties:\n",
      " _CudaDeviceProperties(name='NVIDIA GeForce RTX 2070', major=7, minor=5, total_memory=8191MB, multi_processor_count=36)\n",
      "Mon Mar 20 18:31:07 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 526.98       Driver Version: 526.98       CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
      "|  0%   47C    P2    49W / 175W |    263MiB /  8192MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4756    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A      5488    C+G   ...2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A      5996    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
      "|    0   N/A  N/A      6588    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A      8412    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
      "|    0   N/A  N/A      8680      C   ...ython\\Python39\\python.exe    N/A      |\n",
      "|    0   N/A  N/A      8848    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
      "|    0   N/A  N/A     10808    C+G   ...e31\\Documents\\AnyDesk.exe    N/A      |\n",
      "|    0   N/A  N/A     12440    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     13344    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
      "|    0   N/A  N/A     14192    C+G   ...t\\Teams\\current\\Teams.exe    N/A      |\n",
      "|    0   N/A  N/A     14520    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     17772    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
      "|    0   N/A  N/A     20360    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print('Python version:', sys.version)\n",
    "print('CUDA Available:', torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU Name:', torch.cuda.get_device_name())\n",
    "    print('GPU Properties:\\n', torch.cuda.get_device_properties('cuda'))\n",
    "    device = \"cuda\"\n",
    "    torch.cuda.set_per_process_memory_fraction(0.95, 0)\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    print(\"Cuda is not available, please use cpu instead\")\n",
    "    device = \"cpu\"\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d03cfbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 41\n",
    "# Define custom dataset\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, training_csv = None, training_dir = None, transform = None):\n",
    "        # set random seed for FaceDataset\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        # create contructors\n",
    "        self.unique_img_name = None\n",
    "        self.data = dict()\n",
    "        self.images = list()\n",
    "        # label to indices\n",
    "        self.label_to_indices = dict()\n",
    "        self.labels = list()\n",
    "\n",
    "        # read csv file\n",
    "        self.train_df = pd.read_csv(training_csv)\n",
    "        #get the length of entire dataset\n",
    "        self.len_train = len(self.train_df)\n",
    "        # set the transformation\n",
    "        self.transform = transform\n",
    "        # set the train directory\n",
    "        self.train_dir = training_dir\n",
    "        # group each identity together and create list of each identity imgs\n",
    "        self.train_df = self.train_df.groupby('identity')['filename'].apply(list).reset_index().rename({'filename': 'filenames'}, axis = 1)\n",
    "        # load imgs\n",
    "        self.load_imgs(self.train_df)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len_train\n",
    "\n",
    "    # get each pair of images -> 1: same identity, 0: different identity \n",
    "    # if index is even -> same pair \n",
    "    # if index is odd -> random identity\n",
    "    def __getitem__(self, idx):\n",
    "        anchor_img = self.images[idx]\n",
    "        anchor_label = self.labels[idx]\n",
    "\n",
    "        pos_idx = np.random.choice(np.arange(len(self.images))[self.labels == anchor_label])\n",
    "        neg_idx = np.random.choice(np.arange(len(self.images))[self.labels != anchor_label])\n",
    "\n",
    "        pos_img = self.images[pos_idx]\n",
    "        neg_img = self.images[neg_idx]\n",
    "\n",
    "        pos_label = self.labels[pos_idx]\n",
    "        neg_label = self.labels[neg_idx]\n",
    "\n",
    "        if self.transform is None:\n",
    "            img_to_tensor = transforms.ToTensor()\n",
    "            anchor_img = img_to_tensor(anchor_img)\n",
    "            pos_img = img_to_tensor(pos_img)\n",
    "            neg_img = img_to_tensor(neg_img)\n",
    "        else:\n",
    "            anchor_img = self.transform(anchor_img)\n",
    "            pos_img = self.transform(pos_img)\n",
    "            neg_img = self.transform(neg_img)\n",
    "\n",
    "        return anchor_img, pos_img, neg_img\n",
    "\n",
    "    def load_imgs(self, df):\n",
    "        # iterate thought each row\n",
    "        for i, row in df.iterrows():\n",
    "            # get identity of each row\n",
    "            row_identities = row['identity']\n",
    "            # append each identity to numberical value\n",
    "            self.label_to_indices[row_identities] = i\n",
    "            # loop imgs in each identity\n",
    "            for img_name in row['filenames']:\n",
    "                # concatenate the directoru and image name\n",
    "                path_to_image = self.train_dir+img_name\n",
    "                # open image and convert to RGB\n",
    "                img = Image.open(path_to_image).convert('RGB')\n",
    "                    \n",
    "                self.images.append(img)\n",
    "                self.labels.append(i)\n",
    "\n",
    "        self.labels = np.array(self.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e85575a",
   "metadata": {},
   "source": [
    "# Preperation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7799a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 112\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                         std=[0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "train_batch_size = 64\n",
    "val_batch_size = 64\n",
    "\n",
    "train_triplet_dataset = FaceDataset(training_csv = \"./large_prepared_data/train/label_df.csv\", training_dir=\"./large_prepared_data/train/\", transform = train_transform)\n",
    "train_triplet_dataloader = DataLoader(train_triplet_dataset, batch_size=train_batch_size, shuffle=True, pin_memory=True)\n",
    "\n",
    "val_triplet_dataset = FaceDataset(training_csv = \"./large_prepared_data/val/label_df.csv\", training_dir=\"./large_prepared_data/val/\", transform = val_transform)\n",
    "val_triplet_dataloader = DataLoader(val_triplet_dataset, batch_size=val_batch_size, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec1c881",
   "metadata": {},
   "source": [
    "# Triplet Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2ff8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear, Conv2d, BatchNorm1d, BatchNorm2d, PReLU, ReLU, ReLU6, Sigmoid, Dropout2d, Dropout, AvgPool2d, MaxPool2d, AdaptiveAvgPool2d, Sequential, Module, Parameter\n",
    "\n",
    "class Flatten(Module):\n",
    "    def forward(self, input):\n",
    "        return input.view(input.size(0), -1)\n",
    "\n",
    "def l2_norm(input,axis=1):\n",
    "    norm = torch.norm(input,2,axis,True)\n",
    "    output = torch.div(input, norm)\n",
    "    return output\n",
    "\n",
    "class h_sigmoid(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_sigmoid, self).__init__()\n",
    "        self.relu = ReLU6(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(x + 3) / 6\n",
    "\n",
    "\n",
    "class h_swish(Module):\n",
    "    def __init__(self, inplace=True):\n",
    "        super(h_swish, self).__init__()\n",
    "        self.sigmoid = h_sigmoid(inplace=inplace)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x * self.sigmoid(x)\n",
    "\n",
    "\n",
    "class SELayer(Module):\n",
    "    def __init__(self, channel, reduction=4):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = AdaptiveAvgPool2d(1)\n",
    "        self.fc = Sequential(\n",
    "            Linear(channel, channel // reduction),\n",
    "            ReLU(inplace=True),\n",
    "            Linear(channel // reduction, channel),\n",
    "            h_sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class PermutationBlock(Module):\n",
    "    def __init__(self, groups):\n",
    "        super(PermutationBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, input):\n",
    "        n, c, h, w = input.size()\n",
    "        G = self.groups\n",
    "        output = input.view(n, G, c // G, h, w).permute(0, 2, 1, 3, 4).contiguous().view(n, c, h, w)\n",
    "        return output\n",
    "\n",
    "class Conv_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "        self.prelu = PReLU(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.prelu(x)\n",
    "        return x\n",
    "\n",
    "class Linear_block(Module):\n",
    "    def __init__(self, in_c, out_c, kernel=(1, 1), stride=(1, 1), padding=(0, 0), groups=1):\n",
    "        super(Linear_block, self).__init__()\n",
    "        self.conv = Conv2d(in_c, out_channels=out_c, kernel_size=kernel, groups=groups, stride=stride, padding=padding, bias=False)\n",
    "        self.bn = BatchNorm2d(out_c)\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "\n",
    "class Depth_Wise(Module):\n",
    "    def __init__(self, in_c, out_c, residual = False, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=1):\n",
    "        super(Depth_Wise, self).__init__()\n",
    "        self.conv = Conv_block(in_c, out_c=groups, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.conv_dw = Conv_block(groups, groups, groups=groups, kernel=kernel, padding=padding, stride=stride)\n",
    "        self.project = Linear_block(groups, out_c, kernel=(1, 1), padding=(0, 0), stride=(1, 1))\n",
    "        self.residual = residual\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            short_cut = x\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_dw(x)\n",
    "        x = self.project(x)\n",
    "        if self.residual:\n",
    "            output = short_cut + x\n",
    "        else:\n",
    "            output = x\n",
    "        return output\n",
    "\n",
    "class Residual(Module):\n",
    "    def __init__(self, c, num_block, groups, kernel=(3, 3), stride=(1, 1), padding=(1, 1)):\n",
    "        super(Residual, self).__init__()\n",
    "        modules = []\n",
    "        for _ in range(num_block):\n",
    "            modules.append(Depth_Wise(c, c, residual=True, kernel=kernel, padding=padding, stride=stride, groups=groups))\n",
    "        self.model = Sequential(*modules)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b43d1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MobileFaceNet(Module):\n",
    "    def __init__(self, embedding_size=512):\n",
    "        super(MobileFaceNet, self).__init__()\n",
    "        self.conv1 = Conv_block(3, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1))\n",
    "        self.conv2_dw = Conv_block(64, 64, kernel=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)\n",
    "        self.conv_23 = Depth_Wise(64, 64, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=128)\n",
    "        self.conv_3 = Residual(64, num_block=4, groups=128, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_34 = Depth_Wise(64, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=256)\n",
    "        self.conv_4 = Residual(128, num_block=6, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_45 = Depth_Wise(128, 128, kernel=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
    "        self.conv_5 = Residual(128, num_block=2, groups=256, kernel=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.conv_6_sep = Conv_block(128, 512, kernel=(1, 1), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_dw = Linear_block(512, 512, groups=512, kernel=(7,7), stride=(1, 1), padding=(0, 0))\n",
    "        self.conv_6_flatten = Flatten()\n",
    "        self.linear = Linear(512, embedding_size, bias=False)\n",
    "        self.bn = BatchNorm1d(embedding_size)\n",
    "    def forward_once(self,x):\n",
    "        out = self.conv1(x)\n",
    "\n",
    "        out = self.conv2_dw(out)\n",
    "\n",
    "        out = self.conv_23(out)\n",
    "\n",
    "        out = self.conv_3(out)\n",
    "\n",
    "        out = self.conv_34(out)\n",
    "\n",
    "        out = self.conv_4(out)\n",
    "\n",
    "        out = self.conv_45(out)\n",
    "\n",
    "        out = self.conv_5(out)\n",
    "\n",
    "        out = self.conv_6_sep(out)\n",
    "\n",
    "        out = self.conv_6_dw(out)\n",
    "\n",
    "        out = self.conv_6_flatten(out)\n",
    "\n",
    "        out = self.linear(out)\n",
    "\n",
    "        out = self.bn(out)\n",
    "        return l2_norm(out)\n",
    "\n",
    "    def forward(self, anchor_img, positive_img, negative_img):\n",
    "        anchor = self.forward_once(anchor_img)\n",
    "        positive = self.forward_once(positive_img)\n",
    "        negative = self.forward_once(negative_img)\n",
    "        return anchor, positive, negative\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89e7a3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models import resnet34\n",
    "\n",
    "triplet_model = MobileFaceNet()\n",
    "# resnet18 = list(resnet18)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec86e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class TripletNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.Sequential(\n",
    "#             *(resnet18),\n",
    "#             nn.Flatten(),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "#         self.fc1 = nn.Sequential(\n",
    "#\n",
    "#         )\n",
    "#     def forward_once(self, input):\n",
    "#         output = self.layers(input)\n",
    "#         return output\n",
    "#\n",
    "#     def forward(self, anchor_img, positive_img, negative_img):\n",
    "#         anchor = self.forward_once(anchor_img)\n",
    "#         positive = self.forward_once(positive_img)\n",
    "#         negative = self.forward_once(negative_img)\n",
    "#         return anchor, positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aee11b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============================================================================================\n",
      "Layer (type:depth-idx)                        Output Shape              Param #\n",
      "===============================================================================================\n",
      "MobileFaceNet                                 [32, 512]                 --\n",
      "├─Conv_block: 1-1                             [32, 64, 56, 56]          --\n",
      "│    └─Conv2d: 2-1                            [32, 64, 56, 56]          1,728\n",
      "│    └─BatchNorm2d: 2-2                       [32, 64, 56, 56]          128\n",
      "│    └─PReLU: 2-3                             [32, 64, 56, 56]          64\n",
      "├─Conv_block: 1-2                             [32, 64, 56, 56]          --\n",
      "│    └─Conv2d: 2-4                            [32, 64, 56, 56]          576\n",
      "│    └─BatchNorm2d: 2-5                       [32, 64, 56, 56]          128\n",
      "│    └─PReLU: 2-6                             [32, 64, 56, 56]          64\n",
      "├─Depth_Wise: 1-3                             [32, 64, 28, 28]          --\n",
      "│    └─Conv_block: 2-7                        [32, 128, 56, 56]         --\n",
      "│    │    └─Conv2d: 3-1                       [32, 128, 56, 56]         8,192\n",
      "│    │    └─BatchNorm2d: 3-2                  [32, 128, 56, 56]         256\n",
      "│    │    └─PReLU: 3-3                        [32, 128, 56, 56]         128\n",
      "│    └─Conv_block: 2-8                        [32, 128, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-4                       [32, 128, 28, 28]         1,152\n",
      "│    │    └─BatchNorm2d: 3-5                  [32, 128, 28, 28]         256\n",
      "│    │    └─PReLU: 3-6                        [32, 128, 28, 28]         128\n",
      "│    └─Linear_block: 2-9                      [32, 64, 28, 28]          --\n",
      "│    │    └─Conv2d: 3-7                       [32, 64, 28, 28]          8,192\n",
      "│    │    └─BatchNorm2d: 3-8                  [32, 64, 28, 28]          128\n",
      "├─Residual: 1-4                               [32, 64, 28, 28]          --\n",
      "│    └─Sequential: 2-10                       [32, 64, 28, 28]          --\n",
      "│    │    └─Depth_Wise: 3-9                   [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-10                  [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-11                  [32, 64, 28, 28]          18,432\n",
      "│    │    └─Depth_Wise: 3-12                  [32, 64, 28, 28]          18,432\n",
      "├─Depth_Wise: 1-5                             [32, 128, 14, 14]         --\n",
      "│    └─Conv_block: 2-11                       [32, 256, 28, 28]         --\n",
      "│    │    └─Conv2d: 3-13                      [32, 256, 28, 28]         16,384\n",
      "│    │    └─BatchNorm2d: 3-14                 [32, 256, 28, 28]         512\n",
      "│    │    └─PReLU: 3-15                       [32, 256, 28, 28]         256\n",
      "│    └─Conv_block: 2-12                       [32, 256, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-16                      [32, 256, 14, 14]         2,304\n",
      "│    │    └─BatchNorm2d: 3-17                 [32, 256, 14, 14]         512\n",
      "│    │    └─PReLU: 3-18                       [32, 256, 14, 14]         256\n",
      "│    └─Linear_block: 2-13                     [32, 128, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-19                      [32, 128, 14, 14]         32,768\n",
      "│    │    └─BatchNorm2d: 3-20                 [32, 128, 14, 14]         256\n",
      "├─Residual: 1-6                               [32, 128, 14, 14]         --\n",
      "│    └─Sequential: 2-14                       [32, 128, 14, 14]         --\n",
      "│    │    └─Depth_Wise: 3-21                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-22                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-23                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-24                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-25                  [32, 128, 14, 14]         69,632\n",
      "│    │    └─Depth_Wise: 3-26                  [32, 128, 14, 14]         69,632\n",
      "├─Depth_Wise: 1-7                             [32, 128, 7, 7]           --\n",
      "│    └─Conv_block: 2-15                       [32, 512, 14, 14]         --\n",
      "│    │    └─Conv2d: 3-27                      [32, 512, 14, 14]         65,536\n",
      "│    │    └─BatchNorm2d: 3-28                 [32, 512, 14, 14]         1,024\n",
      "│    │    └─PReLU: 3-29                       [32, 512, 14, 14]         512\n",
      "│    └─Conv_block: 2-16                       [32, 512, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-30                      [32, 512, 7, 7]           4,608\n",
      "│    │    └─BatchNorm2d: 3-31                 [32, 512, 7, 7]           1,024\n",
      "│    │    └─PReLU: 3-32                       [32, 512, 7, 7]           512\n",
      "│    └─Linear_block: 2-17                     [32, 128, 7, 7]           --\n",
      "│    │    └─Conv2d: 3-33                      [32, 128, 7, 7]           65,536\n",
      "│    │    └─BatchNorm2d: 3-34                 [32, 128, 7, 7]           256\n",
      "├─Residual: 1-8                               [32, 128, 7, 7]           --\n",
      "│    └─Sequential: 2-18                       [32, 128, 7, 7]           --\n",
      "│    │    └─Depth_Wise: 3-35                  [32, 128, 7, 7]           69,632\n",
      "│    │    └─Depth_Wise: 3-36                  [32, 128, 7, 7]           69,632\n",
      "├─Conv_block: 1-9                             [32, 512, 7, 7]           --\n",
      "│    └─Conv2d: 2-19                           [32, 512, 7, 7]           65,536\n",
      "│    └─BatchNorm2d: 2-20                      [32, 512, 7, 7]           1,024\n",
      "│    └─PReLU: 2-21                            [32, 512, 7, 7]           512\n",
      "├─Linear_block: 1-10                          [32, 512, 1, 1]           --\n",
      "│    └─Conv2d: 2-22                           [32, 512, 1, 1]           25,088\n",
      "│    └─BatchNorm2d: 2-23                      [32, 512, 1, 1]           1,024\n",
      "├─Flatten: 1-11                               [32, 512]                 --\n",
      "├─Linear: 1-12                                [32, 512]                 262,144\n",
      "├─BatchNorm1d: 1-13                           [32, 512]                 1,024\n",
      "├─Conv_block: 1-14                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-24                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-25                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-26                            [32, 64, 56, 56]          (recursive)\n",
      "├─Conv_block: 1-15                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-27                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-28                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-29                            [32, 64, 56, 56]          (recursive)\n",
      "├─Depth_Wise: 1-16                            [32, 64, 28, 28]          (recursive)\n",
      "│    └─Conv_block: 2-30                       [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─Conv2d: 3-37                      [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-38                 [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─PReLU: 3-39                       [32, 128, 56, 56]         (recursive)\n",
      "│    └─Conv_block: 2-31                       [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-40                      [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-41                 [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-42                       [32, 128, 28, 28]         (recursive)\n",
      "│    └─Linear_block: 2-32                     [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Conv2d: 3-43                      [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─BatchNorm2d: 3-44                 [32, 64, 28, 28]          (recursive)\n",
      "├─Residual: 1-17                              [32, 64, 28, 28]          (recursive)\n",
      "│    └─Sequential: 2-33                       [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-45                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-46                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-47                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-48                  [32, 64, 28, 28]          (recursive)\n",
      "├─Depth_Wise: 1-18                            [32, 128, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-34                       [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-49                      [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-50                 [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-51                       [32, 256, 28, 28]         (recursive)\n",
      "│    └─Conv_block: 2-35                       [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-52                      [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-53                 [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-54                       [32, 256, 14, 14]         (recursive)\n",
      "│    └─Linear_block: 2-36                     [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-55                      [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-56                 [32, 128, 14, 14]         (recursive)\n",
      "├─Residual: 1-19                              [32, 128, 14, 14]         (recursive)\n",
      "│    └─Sequential: 2-37                       [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-57                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-58                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-59                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-60                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-61                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-62                  [32, 128, 14, 14]         (recursive)\n",
      "├─Depth_Wise: 1-20                            [32, 128, 7, 7]           (recursive)\n",
      "│    └─Conv_block: 2-38                       [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-63                      [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-64                 [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-65                       [32, 512, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-39                       [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-66                      [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-67                 [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─PReLU: 3-68                       [32, 512, 7, 7]           (recursive)\n",
      "│    └─Linear_block: 2-40                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-69                      [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-70                 [32, 128, 7, 7]           (recursive)\n",
      "├─Residual: 1-21                              [32, 128, 7, 7]           (recursive)\n",
      "│    └─Sequential: 2-41                       [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-71                  [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-72                  [32, 128, 7, 7]           (recursive)\n",
      "├─Conv_block: 1-22                            [32, 512, 7, 7]           (recursive)\n",
      "│    └─Conv2d: 2-42                           [32, 512, 7, 7]           (recursive)\n",
      "│    └─BatchNorm2d: 2-43                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─PReLU: 2-44                            [32, 512, 7, 7]           (recursive)\n",
      "├─Linear_block: 1-23                          [32, 512, 1, 1]           (recursive)\n",
      "│    └─Conv2d: 2-45                           [32, 512, 1, 1]           (recursive)\n",
      "│    └─BatchNorm2d: 2-46                      [32, 512, 1, 1]           (recursive)\n",
      "├─Flatten: 1-24                               [32, 512]                 --\n",
      "├─Linear: 1-25                                [32, 512]                 (recursive)\n",
      "├─BatchNorm1d: 1-26                           [32, 512]                 (recursive)\n",
      "├─Conv_block: 1-27                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-47                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-48                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-49                            [32, 64, 56, 56]          (recursive)\n",
      "├─Conv_block: 1-28                            [32, 64, 56, 56]          (recursive)\n",
      "│    └─Conv2d: 2-50                           [32, 64, 56, 56]          (recursive)\n",
      "│    └─BatchNorm2d: 2-51                      [32, 64, 56, 56]          (recursive)\n",
      "│    └─PReLU: 2-52                            [32, 64, 56, 56]          (recursive)\n",
      "├─Depth_Wise: 1-29                            [32, 64, 28, 28]          (recursive)\n",
      "│    └─Conv_block: 2-53                       [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─Conv2d: 3-73                      [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-74                 [32, 128, 56, 56]         (recursive)\n",
      "│    │    └─PReLU: 3-75                       [32, 128, 56, 56]         (recursive)\n",
      "│    └─Conv_block: 2-54                       [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-76                      [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-77                 [32, 128, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-78                       [32, 128, 28, 28]         (recursive)\n",
      "│    └─Linear_block: 2-55                     [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Conv2d: 3-79                      [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─BatchNorm2d: 3-80                 [32, 64, 28, 28]          (recursive)\n",
      "├─Residual: 1-30                              [32, 64, 28, 28]          (recursive)\n",
      "│    └─Sequential: 2-56                       [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-81                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-82                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-83                  [32, 64, 28, 28]          (recursive)\n",
      "│    │    └─Depth_Wise: 3-84                  [32, 64, 28, 28]          (recursive)\n",
      "├─Depth_Wise: 1-31                            [32, 128, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-57                       [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─Conv2d: 3-85                      [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-86                 [32, 256, 28, 28]         (recursive)\n",
      "│    │    └─PReLU: 3-87                       [32, 256, 28, 28]         (recursive)\n",
      "│    └─Conv_block: 2-58                       [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-88                      [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-89                 [32, 256, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-90                       [32, 256, 14, 14]         (recursive)\n",
      "│    └─Linear_block: 2-59                     [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-91                      [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-92                 [32, 128, 14, 14]         (recursive)\n",
      "├─Residual: 1-32                              [32, 128, 14, 14]         (recursive)\n",
      "│    └─Sequential: 2-60                       [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-93                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-94                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-95                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-96                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-97                  [32, 128, 14, 14]         (recursive)\n",
      "│    │    └─Depth_Wise: 3-98                  [32, 128, 14, 14]         (recursive)\n",
      "├─Depth_Wise: 1-33                            [32, 128, 7, 7]           (recursive)\n",
      "│    └─Conv_block: 2-61                       [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─Conv2d: 3-99                      [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─BatchNorm2d: 3-100                [32, 512, 14, 14]         (recursive)\n",
      "│    │    └─PReLU: 3-101                      [32, 512, 14, 14]         (recursive)\n",
      "│    └─Conv_block: 2-62                       [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-102                     [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-103                [32, 512, 7, 7]           (recursive)\n",
      "│    │    └─PReLU: 3-104                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─Linear_block: 2-63                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Conv2d: 3-105                     [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─BatchNorm2d: 3-106                [32, 128, 7, 7]           (recursive)\n",
      "├─Residual: 1-34                              [32, 128, 7, 7]           (recursive)\n",
      "│    └─Sequential: 2-64                       [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-107                 [32, 128, 7, 7]           (recursive)\n",
      "│    │    └─Depth_Wise: 3-108                 [32, 128, 7, 7]           (recursive)\n",
      "├─Conv_block: 1-35                            [32, 512, 7, 7]           (recursive)\n",
      "│    └─Conv2d: 2-65                           [32, 512, 7, 7]           (recursive)\n",
      "│    └─BatchNorm2d: 2-66                      [32, 512, 7, 7]           (recursive)\n",
      "│    └─PReLU: 2-67                            [32, 512, 7, 7]           (recursive)\n",
      "├─Linear_block: 1-36                          [32, 512, 1, 1]           (recursive)\n",
      "│    └─Conv2d: 2-68                           [32, 512, 1, 1]           (recursive)\n",
      "│    └─BatchNorm2d: 2-69                      [32, 512, 1, 1]           (recursive)\n",
      "├─Flatten: 1-37                               [32, 512]                 --\n",
      "├─Linear: 1-38                                [32, 512]                 (recursive)\n",
      "├─BatchNorm1d: 1-39                           [32, 512]                 (recursive)\n",
      "===============================================================================================\n",
      "Total params: 1,200,512\n",
      "Trainable params: 1,200,512\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 21.23\n",
      "===============================================================================================\n",
      "Input size (MB): 14.45\n",
      "Forward/backward pass size (MB): 2347.96\n",
      "Params size (MB): 4.80\n",
      "Estimated Total Size (MB): 2367.21\n",
      "===============================================================================================\n"
     ]
    }
   ],
   "source": [
    "# triplet_model = TripletNetwork()\n",
    "print(summary(triplet_model, input_size=[(32,3,112,112),(32,3,112,112),(32,3,112,112)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9e2b3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Arcface(Module):\n",
    "    # implementation of additive margin softmax loss in https://arxiv.org/abs/1801.05599\n",
    "    def __init__(self, embedding_size=512,  classnum=51332,  s=64., m=0.5):\n",
    "        super(Arcface, self).__init__()\n",
    "        self.classnum = classnum\n",
    "        self.kernel = Parameter(torch.Tensor(embedding_size,classnum))\n",
    "        # initial kernel\n",
    "        self.kernel.data.uniform_(-1, 1).renorm_(2,1,1e-5).mul_(1e5)\n",
    "        self.m = m # the margin value, default is 0.5\n",
    "        self.s = s # scalar value default is 64, see normface https://arxiv.org/abs/1704.06369\n",
    "        self.cos_m = math.cos(m)\n",
    "        self.sin_m = math.sin(m)\n",
    "        self.mm = self.sin_m * m  # issue 1\n",
    "        self.threshold = math.cos(math.pi - m)\n",
    "        print('Arcface head')\n",
    "\n",
    "    def forward(self, embbedings, label):\n",
    "        # weights norm\n",
    "        nB = len(embbedings)\n",
    "        kernel_norm = l2_norm(self.kernel,axis=0)\n",
    "        # cos(theta+m)\n",
    "        cos_theta = torch.mm(embbedings,kernel_norm)\n",
    "#         output = torch.mm(embbedings,kernel_norm)\n",
    "        cos_theta = cos_theta.clamp(-1,1) # for numerical stability\n",
    "        cos_theta_2 = torch.pow(cos_theta, 2)\n",
    "        sin_theta_2 = 1 - cos_theta_2\n",
    "        sin_theta = torch.sqrt(sin_theta_2)\n",
    "        cos_theta_m = (cos_theta * self.cos_m - sin_theta * self.sin_m)\n",
    "        # this condition controls the theta+m should in range [0, pi]\n",
    "        #      0<=theta+m<=pi\n",
    "        #     -m<=theta<=pi-m\n",
    "        cond_v = cos_theta - self.threshold\n",
    "        cond_mask = cond_v <= 0\n",
    "        keep_val = (cos_theta - self.mm) # when theta not in [0,pi], use cosface instead\n",
    "        cos_theta_m[cond_mask] = keep_val[cond_mask]\n",
    "        output = cos_theta * 1.0 # a little bit hacky way to prevent in_place operation on cos_theta\n",
    "        idx_ = torch.arange(0, nB, dtype=torch.long)\n",
    "        output[idx_, label] = cos_theta_m[idx_, label]\n",
    "        output *= self.s # scale up in order to make softmax work, first introduced in normface\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69562b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, anchor, positive, negative, size_average=True):\n",
    "        distance_positive = (anchor - positive).pow(2).sum(1)\n",
    "        distance_negative = (anchor - negative).pow(2).sum(1)  \n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean() if size_average else losses.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc16e92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    'margin': 0.534,\n",
    "    'lr': 1e-4,\n",
    "    'patience': 5,\n",
    "    'factor': 0.1,\n",
    "    'min_lr': 1e-8,\n",
    "    'threshold': 1e-2\n",
    "}\n",
    "criterion = TripletLoss(margin = model_config['margin'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94a3dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "optimizer = optim.Adam(triplet_model.parameters(), lr=model_config['lr'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=model_config['patience'], factor=model_config['factor'], min_lr=model_config['min_lr'], threshold=model_config['threshold'], verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6a8ac6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14422c4535ce42e8bae0b3d0bb8b676e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1/200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4f1aec3ce54bcebdc68a7b59f86f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4691, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4631, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4838, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4707, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4669, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4575, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4869, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4950, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4972, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "tensor(0.4368, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8680/2688372450.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m             \u001b[1;31m# append train loss to epoch_train_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m             \u001b[0mtrain_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0manchor_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[1;31m# calculate loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "load_weight_path = './weights/triplet_net_resnet18.pth'\n",
    "save_weight_path = './weights/triplet_net_resnet18.pth'\n",
    "load_weight = False\n",
    "# set epochs rounds\n",
    "num_epochs = 200\n",
    "# throw model to compute in the device\n",
    "triplet_model.to(device=device)\n",
    "total_train_loss = list()\n",
    "total_val_loss = list()\n",
    "best_val_loss = 0\n",
    "best_train_loss = 0\n",
    "previous_lr = 0\n",
    "\n",
    "# load weight\n",
    "if bool(load_weight_path) & load_weight:\n",
    "    triplet_model.load_state_dict(torch.load(load_weight_path))\n",
    "    print('Congratulations, Weight has been loaded!')\n",
    "\n",
    "    \n",
    "# iterate though each epoch\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    print(f'Epoch:{epoch+1}/{num_epochs}')\n",
    "    epoch_train_loss = list()\n",
    "    train_loss = 0\n",
    "    # switch to train mode\n",
    "    triplet_model.train()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # loop though each item in dataloader\n",
    "        for anchor_img, pos_img, neg_img in tqdm(train_triplet_dataloader):\n",
    "            # throw img to compute in the device\n",
    "            anchor_img = anchor_img.to(device)\n",
    "            pos_img = pos_img.to(device)\n",
    "            neg_img = neg_img.to(device)\n",
    "            # clear gradient to prevent gradient vanish\n",
    "            optimizer.zero_grad()\n",
    "            # train model\n",
    "            output1, output2, output3 = triplet_model(anchor_img, pos_img, neg_img)\n",
    "            # compute loss from criterion\n",
    "            loss = criterion(output1, output2, output3)\n",
    "            print(loss)\n",
    "            # backward propagate\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # append train loss to epoch_train_loss\n",
    "            train_loss += loss.item() * anchor_img.size(0)\n",
    "\n",
    "    # calculate loss\n",
    "    current_train_loss = train_loss / len(train_triplet_dataloader.sampler)\n",
    "    total_train_loss.append(current_train_loss)\n",
    "\n",
    "    # switch mode to eval\n",
    "    triplet_model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0\n",
    "        for anchor_img, pos_img, neg_img in tqdm(val_triplet_dataloader):\n",
    "            # throw img to compute in the device\n",
    "            anchor_img = anchor_img.to(device)\n",
    "            pos_img = pos_img.to(device)\n",
    "            neg_img = neg_img.to(device)\n",
    "            # clear gradient to prevent gradient vanish\n",
    "            optimizer.zero_grad()\n",
    "            # compute the out in eval mode\n",
    "            output1, output2, output3 = triplet_model(anchor_img, pos_img, neg_img)\n",
    "            # compute loss in eval mode\n",
    "            loss = criterion(output1, output2, output3)\n",
    "            # calculate loss for each feature\n",
    "            val_loss += loss.item() * anchor_img.size(0)\n",
    "            # calculate loss for each batch\n",
    "    current_val_loss = val_loss / len(val_triplet_dataloader.sampler)\n",
    "    # get learning rate from model\n",
    "    optim_lr = optimizer.param_groups[0]['lr']\n",
    "    # append current validation loss to list\n",
    "    total_val_loss.append(current_val_loss)\n",
    "    if (best_val_loss == 0) | (best_train_loss == 0):\n",
    "        best_val_loss = current_val_loss\n",
    "        best_train_loss = current_train_loss\n",
    "    # find the best val loss\n",
    "    if best_val_loss >= current_val_loss:\n",
    "        best_val_loss = current_val_loss\n",
    "    # find the best train loss\n",
    "    if best_train_loss >= current_train_loss:\n",
    "        best_train_loss = current_train_loss\n",
    "\n",
    "    print(f'train loss: {current_train_loss}')\n",
    "    print(f'val loss: {current_val_loss}')\n",
    "    print(f'Learning rate: {optim_lr:.8f}')\n",
    "    if (optim_lr < previous_lr) | (optim_lr > previous_lr):\n",
    "        print('LEARNING RATE HAS CHANGED!')\n",
    "    print('-------------------------------------------------------------')\n",
    "    # replace the previous lr with the current lr\n",
    "    previous_lr = optim_lr\n",
    "    scheduler.step(current_train_loss)\n",
    "\n",
    "print('Best Validation loss',best_val_loss)\n",
    "print('Best Train loss', best_train_loss)\n",
    "\n",
    "# save model weights\n",
    "torch.save(triplet_model.state_dict(), save_weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d16cf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1,len(total_train_loss)+1), total_train_loss,label = 'Train Loss')\n",
    "plt.plot(np.arange(1,len(total_val_loss)+1),total_val_loss, label= 'Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad75651",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
